{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import package for http requests\n",
    "import requests\n",
    "\n",
    "# import json library to decode and encode Json\n",
    "import json\n",
    "\n",
    "# import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3341b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee306fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Base call for api information for a 22 year old male in arizona\n",
    "# Define the API key and the URL\n",
    "apikey = \"TIcoVtlLge9k99LZO4fyZaqfaf6UILpH\"\n",
    "url = f\"https://marketplace.api.healthcare.gov/api/v1/plans/search?apikey={apikey}\"\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'content-type': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the JSON payload for the POST request\n",
    "data = {\n",
    "    \"household\": {\n",
    "      \"people\": [\n",
    "        {\n",
    "          \"age\": 22,\n",
    "          \"gender\": \"Male\",\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"market\": \"Individual\",\n",
    "    \"place\": {\n",
    "      \"countyfips\": \"04001\",\n",
    "      \"state\": \"AZ\",\n",
    "      \"zipcode\": \"86503\"\n",
    "    },\n",
    "    \"year\": 2019\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "response_data = response.json()\n",
    "\n",
    "# Pretty print the response data\n",
    "print(json.dumps(response_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to figure out how to get more detailed fields\n",
    "\n",
    "# Define the API key and the URL\n",
    "apikey = \"TIcoVtlLge9k99LZO4fyZaqfaf6UILpH\"\n",
    "url = f\"https://marketplace.api.healthcare.gov/api/v1/plans/search?apikey={apikey}\"\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'content-type': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the JSON payload for the POST request to focus on financial information\n",
    "data = {\n",
    "    \"household\": {\n",
    "        \"people\": [\n",
    "            {\n",
    "                \"age\": 22,\n",
    "                \"gender\": \"Male\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"market\": \"Individual\",\n",
    "    \"place\": {\n",
    "        \"countyfips\": \"04001\",\n",
    "        \"state\": \"AZ\",\n",
    "        \"zipcode\": \"86503\"\n",
    "    },\n",
    "    \"year\": 2019,\n",
    "    \"fields\": [\"premiums\", \"deductibles\", \"moops\", \"metal_levels\"]  # Hypothetical parameter to request specific fields\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    response_data = response.json()\n",
    "\n",
    "    # Pretty print the response data focused on financial aspects\n",
    "    print(json.dumps(response_data, indent=4))\n",
    "else:\n",
    "    print(f\"Failed to retrieve data: {response.status_code} {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd487db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial fields captured in single zip\n",
    "\n",
    "# Define the API key and the URL\n",
    "apikey = \"TIcoVtlLge9k99LZO4fyZaqfaf6UILpH\"\n",
    "url = f\"https://marketplace.api.healthcare.gov/api/v1/plans/search?apikey={apikey}\"\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'content-type': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the JSON payload for the POST request to focus on financial information\n",
    "data = {\n",
    "    \"household\": {\n",
    "        \"people\": [\n",
    "            {\n",
    "                \"age\": 22,\n",
    "                \"gender\": \"Male\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"market\": \"Individual\",\n",
    "    \"place\": {\n",
    "        \"countyfips\": \"04001\",\n",
    "        \"state\": \"AZ\",\n",
    "        \"zipcode\": \"86503\"\n",
    "    },\n",
    "    \"year\": 2019\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "# Process the response\n",
    "if response.status_code == 200:\n",
    "    response_data = response.json()\n",
    "\n",
    "    # Convert JSON data into a DataFrame focusing on financial information\n",
    "    financial_data = []\n",
    "    for plan in response_data['plans']:  # Update the key according to your actual JSON response structure\n",
    "        entry = {\n",
    "            \"ID\": plan[\"id\"],\n",
    "            \"Name\": plan[\"name\"],\n",
    "            \"Premium\": plan.get(\"premium\"),  # Safe access using get\n",
    "            \"Metal Level\": plan.get(\"metal_level\"),\n",
    "            \"Deductible\": plan[\"deductibles\"][0][\"amount\"] if plan.get(\"deductibles\") else None,\n",
    "            \"Maximum Out of Pocket\": plan[\"moops\"][0][\"amount\"] if plan.get(\"moops\") else None\n",
    "        }\n",
    "        financial_data.append(entry)\n",
    "\n",
    "    df = pd.DataFrame(financial_data)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "else:\n",
    "    print(f\"Failed to retrieve data: {response.status_code} {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f96761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added more information related in pandas DF related to individaul searched\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the API key and the URL\n",
    "apikey = \"TIcoVtlLge9k99LZO4fyZaqfaf6UILpH\"\n",
    "url = f\"https://marketplace.api.healthcare.gov/api/v1/plans/search?apikey={apikey}\"\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'content-type': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the JSON payload for the POST request to focus on financial information\n",
    "payload = {\n",
    "    \"household\": {\n",
    "        \"people\": [\n",
    "            {\n",
    "                \"age\": 22,\n",
    "                \"gender\": \"Male\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"market\": \"Individual\",\n",
    "    \"place\": {\n",
    "        \"countyfips\": \"04001\",\n",
    "        \"state\": \"AZ\",\n",
    "        \"zipcode\": \"86503\"\n",
    "    },\n",
    "    \"year\": 2019\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    response_data = response.json()\n",
    "\n",
    "    # Convert JSON data into a DataFrame focusing on financial information\n",
    "    financial_data = []\n",
    "    for plan in response_data['plans']:  # Make sure to match with your actual JSON structure\n",
    "        entry = {\n",
    "            \"ID\": plan[\"id\"],\n",
    "            \"Name\": plan[\"name\"],\n",
    "            \"Premium\": plan.get(\"premium\"),\n",
    "            \"Metal Level\": plan.get(\"metal_level\"),\n",
    "            \"Deductible\": plan[\"deductibles\"][0][\"amount\"] if plan.get(\"deductibles\") else None,\n",
    "            \"Maximum Out of Pocket\": plan[\"moops\"][0][\"amount\"] if plan.get(\"moops\") else None,\n",
    "            \"Age\": payload[\"household\"][\"people\"][0][\"age\"],\n",
    "            \"Gender\": payload[\"household\"][\"people\"][0][\"gender\"],\n",
    "            \"County FIPS\": payload[\"place\"][\"countyfips\"],\n",
    "            \"State\": payload[\"place\"][\"state\"],\n",
    "            \"Zipcode\": payload[\"place\"][\"zipcode\"],\n",
    "            \"Year\": payload[\"year\"]\n",
    "        }\n",
    "        financial_data.append(entry)\n",
    "\n",
    "    # Convert python dict to pandas DataFrame\n",
    "    df = pd.DataFrame(financial_data)\n",
    "\n",
    "    # Display the DataFrame, showing the first few rows\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(f\"Failed to retrieve data: {response.status_code} {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#added a ton of finacial data\n",
    "# Define the API key and the URL\n",
    "apikey = \"TIcoVtlLge9k99LZO4fyZaqfaf6UILpH\"\n",
    "url = f\"https://marketplace.api.healthcare.gov/api/v1/plans/search?apikey={apikey}\"\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'content-type': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the JSON payload for the POST request\n",
    "payload = {\n",
    "    \"household\": {\n",
    "        \"people\": [\n",
    "            {\n",
    "                \"age\": 22,\n",
    "                \"gender\": \"Male\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"market\": \"Individual\",\n",
    "    \"place\": {\n",
    "        \"countyfips\": \"04001\",\n",
    "        \"state\": \"AZ\",\n",
    "        \"zipcode\": \"86503\"\n",
    "    },\n",
    "    \"year\": 2019\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "# Process the response\n",
    "if response.status_code == 200:\n",
    "    response_data = response.json()\n",
    "\n",
    "    # Convert JSON data into a DataFrame focusing on financial and other details\n",
    "    financial_data = []\n",
    "    for plan in response_data['plans']:\n",
    "        # Extract first deductible and MOOP entries safely\n",
    "        first_deductible = plan[\"deductibles\"][0] if plan.get(\"deductibles\") else {}\n",
    "        first_moops = plan[\"moops\"][0] if plan.get(\"moops\") else {}\n",
    "\n",
    "        entry = {\n",
    "            \"ID\": plan[\"id\"],\n",
    "            \"Name\": plan[\"name\"],\n",
    "            \"Premium\": plan.get(\"premium\"),\n",
    "            \"Premium w/Credit\": plan.get(\"premium_w_credit\"),\n",
    "            \"EHB Premium\": plan.get(\"ehb_premium\"),\n",
    "            \"Metal Level\": plan.get(\"metal_level\"),\n",
    "            \"Type\": plan.get(\"type\"),\n",
    "            \"Product Division\": plan.get(\"product_division\"),\n",
    "            \"Benefits URL\": plan.get(\"benefits_url\"),\n",
    "            \"Deductible Type\": first_deductible.get(\"type\"),\n",
    "            \"Deductible Amount\": first_deductible.get(\"amount\"),\n",
    "            \"MOOP Type\": first_moops.get(\"type\"),\n",
    "            \"MOOP Amount\": first_moops.get(\"amount\"),\n",
    "            \"Age\": payload[\"household\"][\"people\"][0][\"age\"],\n",
    "            \"Gender\": payload[\"household\"][\"people\"][0][\"gender\"],\n",
    "            \"County FIPS\": payload[\"place\"][\"countyfips\"],\n",
    "            \"State\": payload[\"place\"][\"state\"],\n",
    "            \"Zipcode\": payload[\"place\"][\"zipcode\"],\n",
    "            \"Year\": payload[\"year\"]\n",
    "        }\n",
    "        financial_data.append(entry)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(financial_data)\n",
    "else:\n",
    "    print(f\"Failed to retrieve data: {response.status_code} {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorganized data, added more items to the list, added row and column count\n",
    "\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the API key and the URL\n",
    "apikey = \"TIcoVtlLge9k99LZO4fyZaqfaf6UILpH\"\n",
    "url = f\"https://marketplace.api.healthcare.gov/api/v1/plans/search?apikey={apikey}\"\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'content-type': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the JSON payload for the POST request\n",
    "payload = {\n",
    "    \"household\": {\n",
    "        \"people\": [\n",
    "            {\n",
    "                \"age\": 22,\n",
    "                \"gender\": \"Male\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"market\": \"Individual\",\n",
    "    \"place\": {\n",
    "        \"countyfips\": \"04001\",\n",
    "        \"state\": \"AZ\",\n",
    "        \"zipcode\": \"86503\"\n",
    "    },\n",
    "    \"year\": 2019\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "# Process the response\n",
    "if response.status_code == 200:\n",
    "    response_data = response.json()\n",
    "\n",
    "    # Convert JSON data into a DataFrame focusing on financial and other details\n",
    "    financial_data = []\n",
    "    for plan in response_data['plans']:\n",
    "        # Extract first deductible and MOOP entries safely\n",
    "        first_deductible = plan[\"deductibles\"][0] if plan.get(\"deductibles\") else {}\n",
    "        first_moops = plan[\"moops\"][0] if plan.get(\"moops\") else {}\n",
    "        \n",
    "        # Initialize list to collect benefits details\n",
    "        benefits_list = []\n",
    "        for benefit in plan.get(\"benefits\", []):\n",
    "            for cost_sharing in benefit.get(\"cost_sharings\", []):\n",
    "                benefits_list.append({\n",
    "                    \"Benefit Name\": benefit[\"name\"],\n",
    "                    \"Covered\": benefit[\"covered\"],\n",
    "                    \"Coinsurance Rate\": cost_sharing.get(\"coinsurance_rate\"),\n",
    "                    \"Coinsurance Options\": cost_sharing.get(\"coinsurance_options\"),\n",
    "                    \"Copay Amount\": cost_sharing.get(\"copay_amount\"),\n",
    "                    \"Copay Options\": cost_sharing.get(\"copay_options\"),\n",
    "                    \"Network Tier\": cost_sharing.get(\"network_tier\"),\n",
    "                    \"Cost Sharing Reduction (CSR)\": cost_sharing.get(\"csr\").replace(\"CSR\", \"Cost Sharing Reduction\"),\n",
    "                    \"Display String\": cost_sharing.get(\"display_string\"),\n",
    "                    \"Has Limits\": benefit.get(\"has_limits\"),\n",
    "                    \"Limit Unit\": benefit.get(\"limit_unit\"),\n",
    "                    \"Limit Quantity\": benefit.get(\"limit_quantity\")\n",
    "                })\n",
    "\n",
    "        entry = {\n",
    "            \"ID\": plan[\"id\"],\n",
    "            \"Name\": plan[\"name\"],\n",
    "            \"Premium\": plan.get(\"premium\"),\n",
    "            \"Premium w/Credit\": plan.get(\"premium_w_credit\"),\n",
    "            \"EHB Premium\": plan.get(\"ehb_premium\"),\n",
    "            \"Metal Level\": plan.get(\"metal_level\"),\n",
    "            \"Type\": plan.get(\"type\"),\n",
    "            \"Product Division\": plan.get(\"product_division\"),\n",
    "            \"Benefits URL\": plan.get(\"benefits_url\"),\n",
    "            \"Deductible Type\": first_deductible.get(\"type\"),\n",
    "            \"Deductible Amount\": first_deductible.get(\"amount\"),\n",
    "            \"MOOP Type\": first_moops.get(\"type\"),\n",
    "            \"MOOP Amount\": first_moops.get(\"amount\"),\n",
    "            \"Benefits\": benefits_list,\n",
    "            \"Age\": payload[\"household\"][\"people\"][0][\"age\"],\n",
    "            \"Gender\": payload[\"household\"][\"people\"][0][\"gender\"],\n",
    "            \"County FIPS\": payload[\"place\"][\"countyfips\"],\n",
    "            \"State\": payload[\"place\"][\"state\"],\n",
    "            \"Zipcode\": payload[\"place\"][\"zipcode\"],\n",
    "            \"Year\": payload[\"year\"]\n",
    "        }\n",
    "        financial_data.append(entry)\n",
    "\n",
    "    # Convert python dict to pandas DataFrame\n",
    "    df = pd.DataFrame(financial_data)\n",
    "\n",
    "    # Expand the 'Benefits' column into its own DataFrame\n",
    "    benefits_df = pd.json_normalize(df['Benefits'].explode()).dropna().reset_index(drop=True)\n",
    "    combined_df = df.drop(columns=['Benefits']).join(benefits_df)\n",
    "\n",
    "    # Display the DataFrame, showing the first few rows\n",
    "    print(combined_df.head())\n",
    "else:\n",
    "    print(f\"Failed to retrieve data: {response.status_code} {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d1985",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aefb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#added more information and trying to debug small name issue\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the API key and the URL\n",
    "apikey = \"TIcoVtlLge9k99LZO4fyZaqfaf6UILpH\"\n",
    "url = f\"https://marketplace.api.healthcare.gov/api/v1/plans/search?apikey={apikey}\"\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'content-type': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the JSON payload for the POST request\n",
    "payload = {\n",
    "    \"household\": {\n",
    "        \"people\": [\n",
    "            {\n",
    "                \"age\": 22,\n",
    "                \"gender\": \"Male\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"market\": \"Individual\",\n",
    "    \"place\": {\n",
    "        \"countyfips\": \"04001\",\n",
    "        \"state\": \"AZ\",\n",
    "        \"zipcode\": \"86544\"\n",
    "    },\n",
    "    \"year\": 2019\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "# Process the response\n",
    "if response.status_code == 200:\n",
    "    response_data = response.json()\n",
    "\n",
    "    # Convert JSON data into a DataFrame focusing on financial and other details\n",
    "    financial_data = []\n",
    "    for plan in response_data['plans']:\n",
    "        # Initialize list to collect benefits details\n",
    "        benefits_list = []\n",
    "        for benefit in plan.get(\"benefits\", []):\n",
    "            benefits_list.append({\n",
    "                \"Benefit Name\": benefit[\"name\"],\n",
    "                \"Covered\": benefit[\"covered\"],\n",
    "                \"Has Limits\": benefit.get(\"has_limits\"),\n",
    "                \"Limit Unit\": benefit.get(\"limit_unit\"),\n",
    "                \"Limit Quantity\": benefit.get(\"limit_quantity\")\n",
    "            })\n",
    "        \n",
    "        entry = {\n",
    "            \"ID\": plan[\"id\"],\n",
    "            \"Name\": plan[\"name\"],\n",
    "            \"Premium\": plan.get(\"premium\"),\n",
    "            \"Premium w/Credit\": plan.get(\"premium_w_credit\"),\n",
    "            \"EHB Premium\": plan.get(\"ehb_premium\"),\n",
    "            \"Metal Level\": plan.get(\"metal_level\"),\n",
    "            \"Type\": plan.get(\"type\"),\n",
    "            \"Product Division\": plan.get(\"product_division\"),\n",
    "            \"Benefits URL\": plan.get(\"benefits_url\"),\n",
    "            \"Benefits\": benefits_list,\n",
    "            \"Age\": payload[\"household\"][\"people\"][0][\"age\"],\n",
    "            \"Gender\": payload[\"household\"][\"people\"][0][\"gender\"],\n",
    "            \"County FIPS\": payload[\"place\"][\"countyfips\"],\n",
    "            \"State\": payload[\"place\"][\"state\"],\n",
    "            \"Zipcode\": payload[\"place\"][\"zipcode\"],\n",
    "            \"Year\": payload[\"year\"]\n",
    "        }\n",
    "        financial_data.append(entry)\n",
    "\n",
    "    # Convert python dict to pandas DataFrame\n",
    "    df = pd.DataFrame(financial_data)\n",
    "    benefits_df = pd.json_normalize(df['Benefits'].explode()).dropna().reset_index(drop=True)\n",
    "    combined_df = df.drop(columns=['Benefits']).join(benefits_df)\n",
    "\n",
    "    # Ensure all columns are shown in the output\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    print(combined_df.head())\n",
    "else:\n",
    "    print(f\"Failed to retrieve data: {response.status_code} {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c301cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#expanding api calling by looping through multiple zip codes within FIPS\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the API key and the URL\n",
    "apikey = \"TIcoVtlLge9k99LZO4fyZaqfaf6UILpH\"\n",
    "url = f\"https://marketplace.api.healthcare.gov/api/v1/plans/search?apikey={apikey}\"\n",
    "\n",
    "# List of zip codes and their respective FIPS to iterate over\n",
    "zip_codes = [\n",
    "    85920, 86520, 86502, 86503, 85924, 86535, 85925, 86504, 86505, 85927,\n",
    "    86506, 86033, 86507, 86508, 86538, 85930, 84531, 87328, 86540, 85932,\n",
    "    86028, 86510, 86544, 86545, 86547, 85936, 86511, 86512, 85901, 85938,\n",
    "    86514, 86556, 85940, 85941, 86515\n",
    "]\n",
    "\n",
    "# Initialize list to collect all entries\n",
    "all_data = []\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'content-type': 'application/json'\n",
    "}\n",
    "\n",
    "# Loop over zip codes\n",
    "for zipcode in zip_codes:\n",
    "    # Configure retry mechanism\n",
    "    max_retries = 3\n",
    "    attempts = 0\n",
    "\n",
    "    while attempts < max_retries:\n",
    "        print(f\"Attempting to retrieve data for ZIP code: {zipcode}, attempt: {attempts+1}\")\n",
    "\n",
    "        # Define the JSON payload for the POST request\n",
    "        payload = {\n",
    "            \"household\": {\n",
    "                \"people\": [\n",
    "                    {\n",
    "                        \"age\": 22,\n",
    "                        \"gender\": \"Male\",\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"market\": \"Individual\",\n",
    "            \"place\": {\n",
    "                \"countyfips\": \"04001\",\n",
    "                \"state\": \"AZ\",\n",
    "                \"zipcode\": str(zipcode)\n",
    "            },\n",
    "            \"year\": 2019\n",
    "        }\n",
    "\n",
    "        # Make the POST request\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        attempts += 1  # Increment attempt counter\n",
    "\n",
    "        # Process the response\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Successful data retrieval for ZIP code: {zipcode}\")\n",
    "            response_data = response.json()\n",
    "\n",
    "            # Parse and store data\n",
    "            for plan in response_data.get('plans', []):\n",
    "                entry = {\n",
    "                    \"ID\": plan[\"id\"],\n",
    "                    \"Name\": plan[\"name\"],\n",
    "                    \"Premium\": plan.get(\"premium\"),\n",
    "                    \"Premium w/Credit\": plan.get(\"premium_w_credit\"),\n",
    "                    \"EHB Premium\": plan.get(\"ehb_premium\"),\n",
    "                    \"Metal Level\": plan.get(\"metal_level\"),\n",
    "                    \"Type\": plan.get(\"type\"),\n",
    "                    \"Product Division\": plan.get(\"product_division\"),\n",
    "                    \"Benefits URL\": plan.get(\"benefits_url\"),\n",
    "                    \"Age\": 22,\n",
    "                    \"Gender\": \"Male\",\n",
    "                    \"County FIPS\": payload[\"place\"][\"countyfips\"],\n",
    "                    \"State\": payload[\"place\"][\"state\"],\n",
    "                    \"Zipcode\": zipcode,\n",
    "                    \"Year\": payload[\"year\"]\n",
    "                }\n",
    "                all_data.append(entry)\n",
    "            break  # Exit retry loop on success\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for ZIP code: {zipcode}, status: {response.status_code}, error: {response.text}\")\n",
    "            time.sleep(1)  # Optional: sleep between retries to reduce load on the server\n",
    "\n",
    "# Convert all collected data into a DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b17a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning data and removing incorrect zipcodes for FIPS Apache County \n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the API key and the URL\n",
    "apikey = \"TIcoVtlLge9k99LZO4fyZaqfaf6UILpH\"\n",
    "url = f\"https://marketplace.api.healthcare.gov/api/v1/plans/search?apikey={apikey}\"\n",
    "\n",
    "# List of zip codes and their respective FIPS to iterate over\n",
    "zip_codes = [\n",
    "    85920, 86502, 86503, 85924, 86535, 85925, 86504, 86505, 85927,\n",
    "    86506, 86507, 86508, 86538, 85930, 86540, 85932,\n",
    "    86028, 86544, 86545, 86547, 85936, 86511, 86512, 85938,\n",
    "    86514, 86556, 85940, 86515\n",
    "]\n",
    "\n",
    "# Initialize list to collect all entries and errors\n",
    "all_data = []\n",
    "error_zipcodes = []\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'content-type': 'application/json'\n",
    "}\n",
    "\n",
    "# Loop over zip codes\n",
    "for zipcode in zip_codes:\n",
    "    # Configure retry mechanism\n",
    "    max_retries = 3\n",
    "    attempts = 0\n",
    "    success = False\n",
    "\n",
    "    while attempts < max_retries and not success:\n",
    "        print(f\"Attempting to retrieve data for ZIP code: {zipcode}, attempt: {attempts+1}\")\n",
    "\n",
    "        # Define the JSON payload for the POST request\n",
    "        payload = {\n",
    "            \"household\": {\n",
    "                \"people\": [\n",
    "                    {\n",
    "                        \"age\": 22,\n",
    "                        \"gender\": \"Male\",\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"market\": \"Individual\",\n",
    "            \"place\": {\n",
    "                \"countyfips\": \"04001\",\n",
    "                \"state\": \"AZ\",\n",
    "                \"zipcode\": str(zipcode)\n",
    "            },\n",
    "            \"year\": 2019\n",
    "        }\n",
    "\n",
    "        # Make the POST request\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        attempts += 1  # Increment attempt counter\n",
    "\n",
    "        # Process the response\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Successful data retrieval for ZIP code: {zipcode}\")\n",
    "            response_data = response.json()\n",
    "            success = True\n",
    "\n",
    "            # Parse and store data\n",
    "            for plan in response_data.get('plans', []):\n",
    "                entry = {\n",
    "                    \"ID\": plan[\"id\"],\n",
    "                    \"Name\": plan[\"name\"],\n",
    "                    \"Premium\": plan.get(\"premium\"),\n",
    "                    \"Premium w/Credit\": plan.get(\"premium_w_credit\"),\n",
    "                    \"EHB Premium\": plan.get(\"ehb_premium\"),\n",
    "                    \"Metal Level\": plan.get(\"metal_level\"),\n",
    "                    \"Type\": plan.get(\"type\"),\n",
    "                    \"Product Division\": plan.get(\"product_division\"),\n",
    "                    \"Benefits URL\": plan.get(\"benefits_url\"),\n",
    "                    \"Age\": 22,  # Static as defined in payload\n",
    "                    \"Gender\": \"Male\",  # Static as defined in payload\n",
    "                    \"County FIPS\": payload[\"place\"][\"countyfips\"],\n",
    "                    \"State\": payload[\"place\"][\"state\"],\n",
    "                    \"Zipcode\": zipcode,\n",
    "                    \"Year\": 2019\n",
    "                }\n",
    "                all_data.append(entry)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for ZIP code: {zipcode}, status: {response.status_code}, error: {response.text}\")\n",
    "\n",
    "    if not success:\n",
    "        error_zipcodes.append(zipcode)\n",
    "\n",
    "# Convert all collected data into a DataFrame\n",
    "converted_df = pd.DataFrame(all_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Print zip codes that resulted in errors\n",
    "print(\"Zip codes that resulted in errors:\", error_zipcodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5768065",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2258a01c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initial big data pull\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the API key and the URL\n",
    "apikey = \"TIcoVtlLge9k99LZO4fyZaqfaf6UILpH\"\n",
    "url = f\"https://marketplace.api.healthcare.gov/api/v1/plans/search?apikey={apikey}\"\n",
    "\n",
    "# County ZIP codes and their respective FIPS codes\n",
    "counties = {\n",
    "    \"Apache\": {\"fips\": \"04001\", \"zip_codes\": [85920, 86502, 86503, 85924, 86535, 85925, 86504, 86505, 85927, 86506, \n",
    "                                              86507, 86508, 86538, 85930, 86540, 85932, 86028, 86544, 86545, 86547, \n",
    "                                              85936, 86511, 86512, 85938, 86514, 86556, 85940, 86515]},\n",
    "    \n",
    "    \"Cochise\": {\"fips\": \"04003\", \"zip_codes\": [85635, 85607, 85650, 85602, 85643, 85615, 85603, 85616, 85613, 85611, \n",
    "                                               85630, 85617, 85610, 85638, 85625, 85606, 85632, 85605, 85609, 85626, \n",
    "                                               85620, 85608, 85627, 85636, 85644, 85655, 85670, 85671]},\n",
    "    \n",
    "    \"Coconino\": {\"fips\": \"04005\", \"zip_codes\": [86001, 86004, 86005, 86045, 86336, 86040, 86046, 86351, 86011, 86053, \n",
    "                                                85544, 86035, 86020, 86337, 86052, 86016, 86023, \n",
    "                                                86036, 86024, 86435, 86018, 86015, 86002, 86003, \n",
    "                                                86017, 86339]},\n",
    "    \n",
    "    \"Gila\": {\"fips\": \"04007\", \"zip_codes\": [85541, 85501, 85550, 85539, 85544, 85192, 85554, 85926, \n",
    "                                            85553, 85532, 85235, 85502, 85547]},\n",
    "    \n",
    "    \"Graham\": {\"fips\": \"04009\", \"zip_codes\": [85546, 85643, 85552, 85550, 85543, 85533, 85536, 85530, 85531, \n",
    "                                              85535, 85548, 85551]},\n",
    "    \n",
    "    \"Greenlee\": {\"fips\": \"04011\", \"zip_codes\": [85533, 85540, 85922]},\n",
    "    \n",
    "    \"La Paz\": {\"fips\": \"04012\", \"zip_codes\": [85344, 85346, 85348, 85325, 85334, 85357, 85328, 85371, 85359]},\n",
    "    \n",
    "    \"Maricopa\": {\"fips\": \"04013\", \"zip_codes\": [85142, 85225, 85032, 85326, 85204, 85301, 85383, 85308, 85041, 85345, \n",
    "                                                85033, 85008, 85338, 85035, 85009, 85282, 85234, 85037, 85207, \n",
    "                                                85022, 85201, 85379, 85286, 85029, 85295, 85296, 85224, 85212, 85281, \n",
    "                                                85323, 85042, 85254, 85353, 85205, 85051, 85283, 85015,  \n",
    "                                                85202, 85255, 85382, 85086, 85017, 85044, 85209, 85374, 85213, 85021, \n",
    "                                                85210, 85298, 85251, 85027, 85297, 85260, 85392, 85233, 85303, 85302, \n",
    "                                                85203, 85340, 85208, 85018, 85248, 85206, 85043, 85395, 85016, 85023, \n",
    "                                                85040, 85335, 85048, 85388, 85020, 85031, 85331, 85050, 85257, 85351, \n",
    "                                                85375, 85019, 85053, 85304, 85381, 85014, 85396, 85085, 85024, 85006, \n",
    "                                                85306, 85258, 85268, 85259, 85284, 85013, 85139, 85028, 85310, \n",
    "                                                85083, 85373, 85253, 85215, 85250, 85387, 85007, 85305, 85355, 85307, \n",
    "                                                85262, 85266, 85378, 85004, 85087, 85054, 85012, 85361, 85003, \n",
    "                                                85390, 85354, 85256, 85034, 85045, 85363, 85289, \n",
    "                                                85337, 85263, 85264, 85322, 85329, 85309, 85287, \n",
    "                                                85333, 85377, 85097, 85099, 85096, 85290, 85380, 85385, 85318, 85327, \n",
    "                                                85343, 85358, 85372, 85376, \n",
    "                                                85236, 85244, 85246, 85252, 85261, 85267, 85271, 85269, 85275, 85274, \n",
    "                                                85277, 85280, 85285, 85299, 85311, 85312, 85001, 85002, 85005, 85011, \n",
    "                                                85010, 85026, 85030, 85036, 85039, 85038, 85046, 85060, 85062, 85061, \n",
    "                                                85064, 85063, 85066, 85065, 85068, 85067, 85070, 85069, 85072, 85071, \n",
    "                                                85074, 85073, 85076, 85075, 85078, 85080, 85079, 85082, 85211, 85214, \n",
    "                                                85216, 85190, 85127]},\n",
    "    \n",
    "    \"Mohave\": {\"fips\": \"04015\", \"zip_codes\": [86442, 86401, 86409, 86406, 86403, 86404, 86426, 86413, 86440, 86429, \n",
    "                                              86021, 86432, 86441, 86434, 86444, 85360, 86436, 86443, 86445, \n",
    "                                              86411, 86438, 86437, 86431, 86405, 86412, 86427, 86430, 86433, 86439, \n",
    "                                              86446, 86402]},\n",
    "    \n",
    "    \"Navajo\": {\"fips\": \"04017\", \"zip_codes\": [85901, 86047, 85929, 85937, 86033, 86025, 85928, 86510, \n",
    "                                              85935, 86039, 86520, 86034, 86031, 86054, 85911, 85933, 86032, \n",
    "                                              85926, 86042, 85942, 85934, 86029, 86043, 85902, 85912, \n",
    "                                              85923, 85939]},\n",
    "    \n",
    "    \"Pima\": {\"fips\": \"04019\", \"zip_codes\": [85705, 85706, 85710, 85719, 85746, 85713, 85711, 85756, 85745, 85730, \n",
    "                                            85741, 85704, 85629, 85716, 85743, 85712, 85742, 85747, 85718, 85641, \n",
    "                                            85750, 85737, 85715, 85653, 85757, 85748, 85749, 85714, \n",
    "                                            85658, 85739, 85735, 85602, 85634, 85736, 85622, 85701, 85611, \n",
    "                                            85708, 85707, 85709, 85619, 85637, 85723, 85744, 85633, \n",
    "                                            85639, 85601, 85724, 85341, 85734, 85738, 85740, 85751, 85754, 85752, \n",
    "                                            85775, 85652, 85654, 85703, 85702, 85717, 85721, 85720, 85722, 85725, \n",
    "                                            85728, 85726, 85731, 85733, 85732]},\n",
    "    \n",
    "    \"Pinal\": {\"fips\": \"04021\", \"zip_codes\": [85142, 85122, 85212, 85138, 85143, 85132, 85140, \n",
    "                                             85248, 85120, 85119, 85131, 85139, 85128, 85658, 85739, \n",
    "                                             85123, 85194, 85193, 85623, 85631, 85539, \n",
    "                                             85173, 85137, 85147, 85618, 85192, 85172,  \n",
    "                                             85293, 85294, 85220, 85191, 85141, 85121, 85130, \n",
    "                                             85117, 85178, 85145]},\n",
    "    \n",
    "    \"Santa Cruz\": {\"fips\": \"04023\", \"zip_codes\": [85621, 85648, 85611, 85624, 85646, 85637, 85640, \n",
    "                                                  85628]},\n",
    "    \n",
    "    \"Yavapai\": {\"fips\": \"04025\", \"zip_codes\": [86314, 86326, 86301, 86305, 86303, 86323, 86322, 86336, 86315, 86327, \n",
    "                                               85390, 86351, 86333, 86325, 86334, 86335, 86324, 85324, 86332, \n",
    "                                               85544, 86321, 86337, 85332, 86338, 86343, 85362, \n",
    "                                               86331, 86329, 86302, 86304, 86312, 86313, 86340, 86342, 86341]},\n",
    "    \n",
    "    \"Yuma\": {\"fips\": \"04027\", \"zip_codes\": [85364, 85350, 85349, 85367,85356, 85347, 85333, 85336, 85352, \n",
    "                                            85366, 85369]}\n",
    "}\n",
    "\n",
    "# Initialize list to collect all entries and errors\n",
    "all_data = []\n",
    "failed_zip_codes = []\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'content-type': 'application/json'\n",
    "}\n",
    "\n",
    "# Loop over each county and their zip codes\n",
    "for county, info in counties.items():\n",
    "    fips = info['fips']\n",
    "    for zipcode in info['zip_codes']:\n",
    "        attempts = 0\n",
    "        max_retries = 1\n",
    "        while attempts < max_retries:\n",
    "            print(f\"Attempting to retrieve data for {county} County, ZIP code: {zipcode}, attempt: {attempts + 1}\")\n",
    "            payload = {\n",
    "                \"household\": {\n",
    "                    \"people\": [{\"age\": 22, \"gender\": \"Male\"}]\n",
    "                },\n",
    "                \"market\": \"Individual\",\n",
    "                \"place\": {\"countyfips\": fips, \"state\": \"AZ\", \"zipcode\": str(zipcode)},\n",
    "                \"year\": 2019\n",
    "            }\n",
    "            response = requests.post(url, headers=headers, json=payload)\n",
    "            attempts += 1\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Successful data retrieval for ZIP code: {zipcode}\")\n",
    "                response_data = response.json()\n",
    "                for plan in response_data.get('plans', []):\n",
    "                    entry = {\n",
    "                        \"ID\": plan[\"id\"],\n",
    "                        \"Name\": plan[\"name\"],\n",
    "                        \"Premium\": plan.get(\"premium\"),\n",
    "                        \"Premium w/Credit\": plan.get(\"premium_w_credit\"),\n",
    "                        \"EHB Premium\": plan.get(\"ehb_premium\"),\n",
    "                        \"Metal Level\": plan.get(\"metal_level\"),\n",
    "                        \"Type\": plan.get(\"type\"),\n",
    "                        \"Product Division\": plan.get(\"product_division\"),\n",
    "                        \"Benefits URL\": plan.get(\"benefits_url\"),\n",
    "                        \"Age\": 22,\n",
    "                        \"Gender\": \"Male\",\n",
    "                        \"County FIPS\": fips,\n",
    "                        \"State\": \"AZ\",\n",
    "                        \"Zipcode\": zipcode,\n",
    "                        \"Year\": 2019\n",
    "                    }\n",
    "                    all_data.append(entry)\n",
    "                break  # Exit retry loop on success\n",
    "            else:\n",
    "                print(f\"Failed to retrieve data for ZIP code: {zipcode}, status: {response.status_code}, error: {response.text}\")\n",
    "                if attempts == max_retries:\n",
    "                    failed_zip_codes.append(zipcode)\n",
    "                time.sleep(1)  # Sleep between retries\n",
    "\n",
    "# Convert all collected data into a DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Print the DataFrame and failed ZIP codes\n",
    "print(df)\n",
    "print(\"Failed ZIP codes:\", failed_zip_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the API key and the URL\n",
    "apikey = \"TIcoVtlLge9k99LZO4fyZaqfaf6UILpH\"\n",
    "url = f\"https://marketplace.api.healthcare.gov/api/v1/plans/search?apikey={apikey}\"\n",
    "\n",
    "# County ZIP codes and their respective FIPS codes\n",
    "counties = {\n",
    "    \"Apache\": {\"fips\": \"04001\", \"zip_codes\": [85920, 86502, 86503, 85924, 86535, 85925, 86504, 86505, 85927, 86506, \n",
    "                                              86507, 86508, 86538, 85930, 86540, 85932, 86028, 86544, 86545, 86547, \n",
    "                                              85936, 86511, 86512, 85938, 86514, 86556, 85940, 86515]},\n",
    "    \n",
    "    \"Cochise\": {\"fips\": \"04003\", \"zip_codes\": [85635, 85607, 85650, 85602, 85643, 85615, 85603, 85616, 85613, 85611, \n",
    "                                               85630, 85617, 85610, 85638, 85625, 85606, 85632, 85605, 85609, 85626, \n",
    "                                               85620, 85608, 85627, 85636, 85644, 85655, 85670, 85671]},\n",
    "    \n",
    "    \"Coconino\": {\"fips\": \"04005\", \"zip_codes\": [86001, 86004, 86005, 86045, 86336, 86040, 86046, 86011, 86053, \n",
    "                                                86044, 86022, 86035, 86020, 86052, 86016, 86023, \n",
    "                                                86036, 86024, 85931, 86435, 86018, 86038, 86015, 86002, 86003, \n",
    "                                                86017, 86339]},\n",
    "    \n",
    "    \"Gila\": {\"fips\": \"04007\", \"zip_codes\": [85541, 85501, 85550, 85539, 85544, 85192, 85554, \n",
    "                                            85553, 85135, 85532, 85502, 85547]},\n",
    "    \n",
    "    \"Graham\": {\"fips\": \"04009\", \"zip_codes\": [85546, 85643, 85552, 85543, 85536, 85530, 85531, \n",
    "                                              85535, 85548, 85551]},\n",
    "    \n",
    "    \"Greenlee\": {\"fips\": \"04011\", \"zip_codes\": [85533, 85540, 85534, 85922]},\n",
    "    \n",
    "    \"La Paz\": {\"fips\": \"04012\", \"zip_codes\": [85344, 85346, 85348, 85325, 85334, 85357, 85328, 85371, 85359]},\n",
    "    \n",
    "    \"Maricopa\": {\"fips\": \"04013\", \"zip_codes\": [85142, 85225, 85032, 85326, 85204, 85301, 85383, 85308, 85041, 85345, \n",
    "                                                85033, 85008, 85338, 85035, 85009, 85282, 85234, 85339, 85037, 85207, \n",
    "                                                85022, 85201, 85379, 85286, 85029, 85295, 85296, 85224, 85212, 85281, \n",
    "                                                85323, 85042, 85254, 85353, 85249, 85205, 85051, 85283, 85015, 85226, \n",
    "                                                85202, 85255, 85382, 85086, 85017, 85044, 85209, 85374, 85213, 85021, \n",
    "                                                85210, 85298, 85251, 85027, 85297, 85260, 85392, 85233, 85303, 85302, \n",
    "                                                85203, 85340, 85208, 85018, 85248, 85206, 85043, 85395, 85016, 85023, \n",
    "                                                85040, 85335, 85048, 85388, 85020, 85031, 85331, 85050, 85257, 85351, \n",
    "                                                85375, 85019, 85053, 85304, 85381, 85014, 85396, 85085, 85024, 85006, \n",
    "                                                85306, 85258, 85268, 85259, 85284, 85013, 85139, 85028, 85310, \n",
    "                                                85083, 85373, 85253, 85215, 85250, 85387, 85007, 85305, 85355, 85307, \n",
    "                                                85262, 85266, 85378, 85004, 85087, 85054, 85012, 85361, 85003, \n",
    "                                                85390, 85354, 85256, 85034, 85045, 85363, \n",
    "                                                85337, 85263, 85264, 85320, 85322, 85329, 85309, 85287, \n",
    "                                                85333, 85377, \n",
    "                                                85380, 85385, 85318, 85327, 85343, 85358, 85372, 85376, \n",
    "                                                85236, 85244, 85246, 85252, 85261, 85267, 85271, 85269, 85275, 85274, \n",
    "                                                85277, 85280, 85285, 85299, 85311, 85312, 85001, 85002, 85005, 85011, \n",
    "                                                85010, 85026, 85030, 85036, 85039, 85038, 85046, 85060, 85062, 85061, \n",
    "                                                85064, 85063, 85066, 85065, 85068, 85067, 85070, 85069, 85072, 85071, \n",
    "                                                85074, 85073, 85076, 85075, 85078, 85080, 85079, 85082, 85211, 85214, \n",
    "                                                85216, 85190, 85127]},\n",
    "    \n",
    "    \"Mohave\": {\"fips\": \"04015\", \"zip_codes\": [86442, 86401, 86409, 86406, 86403, 86404, 86426, 86413, 86440, 86429, \n",
    "                                              86021, 86432, 86441, 86434, 86444, 85360, 86436, 86443, 86445, \n",
    "                                              86411, 86438, 86437, 86431, 86405, 86412, 86427, 86430, 86433, 86439, \n",
    "                                              86446, 86402]},\n",
    "    \n",
    "    \"Navajo\": {\"fips\": \"04017\", \"zip_codes\": [85901, 86047, 85929, 85937, 86033, 86025, 85928, 86510, \n",
    "                                              85935, 86039, 86520, 86034, 86031, 86054, 85911, 85933, 86032, \n",
    "                                              85926, 86042, 85942, 85934, 86029, 86043, 85902, 85912, \n",
    "                                              85923, 85939]},\n",
    "    \n",
    "    \"Pima\": {\"fips\": \"04019\", \"zip_codes\": [85705, 85706, 85710, 85719, 85746, 85713, 85711, 85756, 85745, 85730, \n",
    "                                            85741, 85704, 85629, 85716, 85743, 85712, 85742, 85747, 85718, 85641, \n",
    "                                            85750, 85614, 85737, 85715, 85653, 85757, 85748, 85749, 85755, 85714, \n",
    "                                            85658, 85739, 85735, 85602, 85634, 85736, 85622, 85701, 85611, \n",
    "                                            85708, 85707, 85645, 85619, 85637, 85723, 85744, 85633, \n",
    "                                            85639, 85601, 85341, 85734, 85738, 85740, 85751, 85754, 85752, \n",
    "                                            85775, 85652, 85654, 85703, 85702, 85717, 85721, 85720, 85722, 85725, \n",
    "                                            85728, 85726, 85731, 85733, 85732]},\n",
    "    \n",
    "    \"Pinal\": {\"fips\": \"04021\", \"zip_codes\": [85142, 85122, 85212, 85138, 85143, 85132, 85140, \n",
    "                                             85248, 85120, 85119, 85131, 85139, 85128, 85658, 85739, \n",
    "                                             85118, 85123, 85194, 85193, 85623, 85631, 85539, \n",
    "                                             85173, 85137, 85147, 85618, 85192, 85172, \n",
    "                                             85191, 85141, 85121, 85130, \n",
    "                                             85117, 85178, 85145]},\n",
    "    \n",
    "    \"Santa Cruz\": {\"fips\": \"04023\", \"zip_codes\": [85621, 85648, 85611, 85624, 85646, 85637, 85640, \n",
    "                                                  85628]},\n",
    "    \n",
    "    \"Yavapai\": {\"fips\": \"04025\", \"zip_codes\": [86314, 86326, 86301, 86305, 86303, 86323, 86322, 86336, 86315, 86327, \n",
    "                                               85390, 86351, 86333, 86325, 86334, 86335, 86324, 85324, 86332, \n",
    "                                               86321, 86337, 85332, 86338, 86343, 85362, \n",
    "                                               86331, 86329, 86302, 86304, 86312, 86313, 86340, 86342, 86341]},\n",
    "    \n",
    "    \"Yuma\": {\"fips\": \"04027\", \"zip_codes\": [85364, 85365, 85350, 85349, 85367, 85356, 85347, 85333, 85336, 85352, \n",
    "                                            85366, 85369]}\n",
    "}\n",
    "\n",
    "# Initialize list to collect all entries and errors\n",
    "all_data = []\n",
    "failed_zip_codes = []\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'content-type': 'application/json'\n",
    "}\n",
    "\n",
    "# Loop over each county and their zip codes\n",
    "for county, info in counties.items():\n",
    "    fips = info['fips']\n",
    "    for zipcode in info['zip_codes']:\n",
    "        attempts = 0\n",
    "        max_retries = 1\n",
    "        while attempts < max_retries:\n",
    "            print(f\"Attempting to retrieve data for {county} County, ZIP code: {zipcode}, attempt: {attempts + 1}\")\n",
    "            payload = {\n",
    "                \"household\": {\n",
    "                    \"people\": [{\"age\": 22, \"gender\": \"Male\"}]\n",
    "                },\n",
    "                \"market\": \"Individual\",\n",
    "                \"place\": {\"countyfips\": fips, \"state\": \"AZ\", \"zipcode\": str(zipcode)},\n",
    "                \"year\": 2019\n",
    "            }\n",
    "            response = requests.post(url, headers=headers, json=payload)\n",
    "            attempts += 1\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Successful data retrieval for ZIP code: {zipcode}\")\n",
    "                response_data = response.json()\n",
    "                for plan in response_data.get('plans', []):\n",
    "                    entry = {\n",
    "                        \"ID\": plan[\"id\"],\n",
    "                        \"Name\": plan[\"name\"],\n",
    "                        \"Premium\": plan.get(\"premium\"),\n",
    "                        \"Premium w/Credit\": plan.get(\"premium_w_credit\"),\n",
    "                        \"EHB Premium\": plan.get(\"ehb_premium\"),\n",
    "                        \"Metal Level\": plan.get(\"metal_level\"),\n",
    "                        \"Type\": plan.get(\"type\"),\n",
    "                        \"Product Division\": plan.get(\"product_division\"),\n",
    "                        \"Benefits URL\": plan.get(\"benefits_url\"),\n",
    "                        \"Age\": 22,\n",
    "                        \"Gender\": \"Male\",\n",
    "                        \"County FIPS\": fips,\n",
    "                        \"State\": \"AZ\",\n",
    "                        \"Zipcode\": zipcode,\n",
    "                        \"Year\": 2019\n",
    "                    }\n",
    "                    all_data.append(entry)\n",
    "                break  # Exit retry loop on success\n",
    "            else:\n",
    "                print(f\"Failed to retrieve data for ZIP code: {zipcode}, status: {response.status_code}, error: {response.text}\")\n",
    "                if attempts == max_retries:\n",
    "                    failed_zip_codes.append(zipcode)\n",
    "                time.sleep(1)  # Sleep between retries\n",
    "\n",
    "# Convert all collected data into a DataFrame\n",
    "big_data_df = pd.DataFrame(all_data)\n",
    "\n",
    "# Print the DataFrame and failed ZIP codes\n",
    "print(df)\n",
    "print(\"Failed ZIP codes:\", failed_zip_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01809d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013f61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#increased dataset to range of 2016-2024 from just 2019, slowed the api call rate down, and created a procedure to continue calls\n",
    "# and created a procedure to continue calls in the event of a timeout\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the API key and the URL\n",
    "apikey = \"TIcoVtlLge9k99LZO4fyZaqfaf6UILpH\"\n",
    "url = f\"https://marketplace.api.healthcare.gov/api/v1/plans/search?apikey={apikey}\"\n",
    "\n",
    "# Define the timeout (in seconds)\n",
    "timeout = 10  # Increase this as needed\n",
    "\n",
    "# County ZIP codes and their respective FIPS codes\n",
    "counties = {\n",
    "    \"Apache\": {\"fips\": \"04001\", \"zip_codes\": [85920, 86502, 86503, 85924, 86535, 85925, 86504, 86505, 85927, 86506, \n",
    "                                              86507, 86508, 86538, 85930, 86540, 85932, 86028, 86544, 86545, 86547, \n",
    "                                              85936, 86511, 86512, 85938, 86514, 86556, 85940, 86515]},\n",
    "    \n",
    "    \"Cochise\": {\"fips\": \"04003\", \"zip_codes\": [85635, 85607, 85650, 85602, 85643, 85615, 85603, 85616, 85613, 85611, \n",
    "                                               85630, 85617, 85610, 85638, 85625, 85606, 85632, 85605, 85609, 85626, \n",
    "                                               85620, 85608, 85627, 85636, 85644, 85655, 85670, 85671]},\n",
    "    \n",
    "    \"Coconino\": {\"fips\": \"04005\", \"zip_codes\": [86001, 86004, 86005, 86045, 86336, 86040, 86046, 86011, 86053, \n",
    "                                                86044, 86022, 86035, 86020, 86052, 86016, 86023, \n",
    "                                                86036, 86024, 85931, 86435, 86018, 86038, 86015, 86002, 86003, \n",
    "                                                86017, 86339]},\n",
    "    \n",
    "    \"Gila\": {\"fips\": \"04007\", \"zip_codes\": [85541, 85501, 85550, 85539, 85544, 85192, 85554, \n",
    "                                            85553, 85135, 85532, 85502, 85547]},\n",
    "    \n",
    "    \"Graham\": {\"fips\": \"04009\", \"zip_codes\": [85546, 85643, 85552, 85543, 85536, 85530, 85531, \n",
    "                                              85535, 85548, 85551]},\n",
    "    \n",
    "    \"Greenlee\": {\"fips\": \"04011\", \"zip_codes\": [85533, 85540, 85534, 85922]},\n",
    "    \n",
    "    \"La Paz\": {\"fips\": \"04012\", \"zip_codes\": [85344, 85346, 85348, 85325, 85334, 85357, 85328, 85371, 85359]},\n",
    "    \n",
    "    \"Maricopa\": {\"fips\": \"04013\", \"zip_codes\": [85142, 85225, 85032, 85326, 85204, 85301, 85383, 85308, 85041, 85345, \n",
    "                                                85033, 85008, 85338, 85035, 85009, 85282, 85234, 85339, 85037, 85207, \n",
    "                                                85022, 85201, 85379, 85286, 85029, 85295, 85296, 85224, 85212, 85281, \n",
    "                                                85323, 85042, 85254, 85353, 85249, 85205, 85051, 85283, 85015, 85226, \n",
    "                                                85202, 85255, 85382, 85086, 85017, 85044, 85209, 85374, 85213, 85021, \n",
    "                                                85210, 85298, 85251, 85027, 85297, 85260, 85392, 85233, 85303, 85302, \n",
    "                                                85203, 85340, 85208, 85018, 85248, 85206, 85043, 85395, 85016, 85023, \n",
    "                                                85040, 85335, 85048, 85388, 85020, 85031, 85331, 85050, 85257, 85351, \n",
    "                                                85375, 85019, 85053, 85304, 85381, 85014, 85396, 85085, 85024, 85006, \n",
    "                                                85306, 85258, 85268, 85259, 85284, 85013, 85139, 85028, 85310, \n",
    "                                                85083, 85373, 85253, 85215, 85250, 85387, 85007, 85305, 85355, 85307, \n",
    "                                                85262, 85266, 85378, 85004, 85087, 85054, 85012, 85361, 85003, \n",
    "                                                85390, 85354, 85256, 85034, 85045, 85363, \n",
    "                                                85337, 85263, 85264, 85320, 85322, 85329, 85309, 85287, \n",
    "                                                85333, 85377, \n",
    "                                                85380, 85385, 85318, 85327, 85343, 85358, 85372, 85376, \n",
    "                                                85236, 85244, 85246, 85252, 85261, 85267, 85271, 85269, 85275, 85274, \n",
    "                                                85277, 85280, 85285, 85299, 85311, 85312, 85001, 85002, 85005, 85011, \n",
    "                                                85010, 85026, 85030, 85036, 85039, 85038, 85046, 85060, 85062, 85061, \n",
    "                                                85064, 85063, 85066, 85065, 85068, 85067, 85070, 85069, 85072, 85071, \n",
    "                                                85074, 85073, 85076, 85075, 85078, 85080, 85079, 85082, 85211, 85214, \n",
    "                                                85216, 85190, 85127]},\n",
    "    \n",
    "    \"Mohave\": {\"fips\": \"04015\", \"zip_codes\": [86442, 86401, 86409, 86406, 86403, 86404, 86426, 86413, 86440, 86429, \n",
    "                                              86021, 86432, 86441, 86434, 86444, 85360, 86436, 86443, 86445, \n",
    "                                              86411, 86438, 86437, 86431, 86405, 86412, 86427, 86430, 86433, 86439, \n",
    "                                              86446, 86402]},\n",
    "    \n",
    "    \"Navajo\": {\"fips\": \"04017\", \"zip_codes\": [85901, 86047, 85929, 85937, 86033, 86025, 85928, 86510, \n",
    "                                              85935, 86039, 86520, 86034, 86031, 86054, 85911, 85933, 86032, \n",
    "                                              85926, 86042, 85942, 85934, 86029, 86043, 85902, 85912, \n",
    "                                              85923, 85939]},\n",
    "    \n",
    "    \"Pima\": {\"fips\": \"04019\", \"zip_codes\": [85705, 85706, 85710, 85719, 85746, 85713, 85711, 85756, 85745, 85730, \n",
    "                                            85741, 85704, 85629, 85716, 85743, 85712, 85742, 85747, 85718, 85641, \n",
    "                                            85750, 85614, 85737, 85715, 85653, 85757, 85748, 85749, 85755, 85714, \n",
    "                                            85658, 85739, 85735, 85602, 85634, 85736, 85622, 85701, 85611, \n",
    "                                            85708, 85707, 85645, 85619, 85637, 85723, 85744, 85633, \n",
    "                                            85639, 85601, 85341, 85734, 85738, 85740, 85751, 85754, 85752, \n",
    "                                            85775, 85652, 85654, 85703, 85702, 85717, 85721, 85720, 85722, 85725, \n",
    "                                            85728, 85726, 85731, 85733, 85732]},\n",
    "    \n",
    "    \"Pinal\": {\"fips\": \"04021\", \"zip_codes\": [85142, 85122, 85212, 85138, 85143, 85132, 85140, \n",
    "                                             85248, 85120, 85119, 85131, 85139, 85128, 85658, 85739, \n",
    "                                             85118, 85123, 85194, 85193, 85623, 85631, 85539, \n",
    "                                             85173, 85137, 85147, 85618, 85192, 85172, \n",
    "                                             85191, 85141, 85121, 85130, \n",
    "                                             85117, 85178, 85145]},\n",
    "    \n",
    "    \"Santa Cruz\": {\"fips\": \"04023\", \"zip_codes\": [85621, 85648, 85611, 85624, 85646, 85637, 85640, \n",
    "                                                  85628]},\n",
    "    \n",
    "    \"Yavapai\": {\"fips\": \"04025\", \"zip_codes\": [86314, 86326, 86301, 86305, 86303, 86323, 86322, 86336, 86315, 86327, \n",
    "                                               85390, 86351, 86333, 86325, 86334, 86335, 86324, 85324, 86332, \n",
    "                                               86321, 86337, 85332, 86338, 86343, 85362, \n",
    "                                               86331, 86329, 86302, 86304, 86312, 86313, 86340, 86342, 86341]},\n",
    "    \n",
    "    \"Yuma\": {\"fips\": \"04027\", \"zip_codes\": [85364, 85365, 85350, 85349, 85367, 85356, 85347, 85333, 85336, 85352, \n",
    "                                            85366, 85369]}\n",
    "}\n",
    "years = list(range(2016, 2025))  # Years from 2016 to 2024\n",
    "\n",
    "# Initialize list to collect all entries and errors\n",
    "all_data = []\n",
    "failed_zip_codes = []\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'content-type': 'application/json'\n",
    "}\n",
    "\n",
    "# Loop over each county and their zip codes\n",
    "for county, info in counties.items():\n",
    "    fips = info['fips']\n",
    "    for zipcode in info['zip_codes']:\n",
    "        for year in years:\n",
    "            attempts = 0\n",
    "            max_retries = 3\n",
    "            while attempts < max_retries:\n",
    "                try:\n",
    "                    print(f\"Attempting to retrieve data for {county} County, ZIP code: {zipcode}, Year: {year}, Attempt: {attempts + 1}\")\n",
    "                    payload = {\n",
    "                        \"household\": {\n",
    "                            \"people\": [{\"age\": 22, \"gender\": \"Male\"}]\n",
    "                        },\n",
    "                        \"market\": \"Individual\",\n",
    "                        \"place\": {\"countyfips\": fips, \"state\": \"AZ\", \"zipcode\": str(zipcode)},\n",
    "                        \"year\": year\n",
    "                    }\n",
    "                    response = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "                    if response.status_code == 200:\n",
    "                        print(f\"Successful data retrieval for ZIP code: {zipcode} in {year}\")\n",
    "                        response_data = response.json()\n",
    "                        for plan in response_data.get('plans', []):\n",
    "                            entry = {\n",
    "                                \"ID\": plan[\"id\"],\n",
    "                                \"Name\": plan[\"name\"],\n",
    "                                \"Premium\": plan.get(\"premium\"),\n",
    "                                \"Premium w/Credit\": plan.get(\"premium_w_credit\"),\n",
    "                                \"EHB Premium\": plan.get(\"ehb_premium\"),\n",
    "                                \"Metal Level\": plan.get(\"metal_level\"),\n",
    "                                \"Type\": plan.get(\"type\"),\n",
    "                                \"Product Division\": plan.get(\"product_division\"),\n",
    "                                \"Benefits URL\": plan.get(\"benefits_url\"),\n",
    "                                \"Age\": 22,\n",
    "                                \"Gender\": \"Male\",\n",
    "                                \"County FIPS\": fips,\n",
    "                                \"State\": \"AZ\",\n",
    "                                \"Zipcode\": zipcode,\n",
    "                                \"Year\": year\n",
    "                            }\n",
    "                            all_data.append(entry)\n",
    "                        break  # Exit retry loop on success\n",
    "                    else:\n",
    "                        print(f\"Failed to retrieve data for ZIP code: {zipcode}, Year: {year}, Status: {response.status_code}, Error: {response.text}\")\n",
    "                        attempts += 1\n",
    "                except requests.exceptions.ConnectTimeout:\n",
    "                    print(f\"Timeout occurred for {zipcode} in {year}\")\n",
    "                    attempts += 1\n",
    "                time.sleep(1)  # Sleep between retries\n",
    "\n",
    "# Convert all collected data into a DataFrame\n",
    "big_data_df = pd.DataFrame(all_data)\n",
    "\n",
    "# Print the DataFrame and failed ZIP codes\n",
    "print(big_data_df)\n",
    "print(\"Failed ZIP codes and years:\", failed_zip_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ebc39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Timeout and retry configurations\n",
    "timeout = 5\n",
    "max_retries = 1\n",
    "\n",
    "# Define the API key and the URL\n",
    "apikey = \"TIcoVtlLge9k99LZO4fyZaqfaf6UILpH\"\n",
    "url = f\"https://marketplace.api.healthcare.gov/api/v1/plans/search?apikey={apikey}\"\n",
    "\n",
    "# Define the county and ZIP codes data\n",
    "# County ZIP codes and their respective FIPS codes\n",
    "counties = {\n",
    "    \"Apache\": {\"fips\": \"04001\", \"zip_codes\": [85920, 86502, 86503, 85924, 86535, 85925, 86504, 86505, 85927, 86506, \n",
    "                                              86507, 86508, 86538, 85930, 86540, 85932, 86028, 86544, 86545, 86547, \n",
    "                                              85936, 86511, 86512, 85938, 86514, 86556, 85940, 86515]},\n",
    "    \n",
    "    \"Cochise\": {\"fips\": \"04003\", \"zip_codes\": [85635, 85607, 85650, 85602, 85643, 85615, 85603, 85616, 85613, 85611, \n",
    "                                               85630, 85617, 85610, 85638, 85625, 85606, 85632, 85605, 85609, 85626, \n",
    "                                               85620, 85608, 85627, 85636, 85644, 85655, 85670, 85671]},\n",
    "    \n",
    "    \"Coconino\": {\"fips\": \"04005\", \"zip_codes\": [86001, 86004, 86005, 86045, 86336, 86040, 86046, 86011, 86053, \n",
    "                                                86044, 86022, 86035, 86020, 86052, 86016, 86023, \n",
    "                                                86036, 86024, 85931, 86435, 86018, 86038, 86015, 86002, 86003, \n",
    "                                                86017, 86339]},\n",
    "    \n",
    "    \"Gila\": {\"fips\": \"04007\", \"zip_codes\": [85541, 85501, 85550, 85539, 85544, 85192, 85554, \n",
    "                                            85553, 85135, 85532, 85502, 85547]},\n",
    "    \n",
    "    \"Graham\": {\"fips\": \"04009\", \"zip_codes\": [85546, 85643, 85552, 85543, 85536, 85530, 85531, \n",
    "                                              85535, 85548, 85551]},\n",
    "    \n",
    "    \"Greenlee\": {\"fips\": \"04011\", \"zip_codes\": [85533, 85540, 85534, 85922]},\n",
    "    \n",
    "    \"La Paz\": {\"fips\": \"04012\", \"zip_codes\": [85344, 85346, 85348, 85325, 85334, 85357, 85328, 85371, 85359]},\n",
    "    \n",
    "    \"Maricopa\": {\"fips\": \"04013\", \"zip_codes\": [85142, 85225, 85032, 85326, 85204, 85301, 85383, 85308, 85041, 85345, \n",
    "                                                85033, 85008, 85338, 85035, 85009, 85282, 85234, 85339, 85037, 85207, \n",
    "                                                85022, 85201, 85379, 85286, 85029, 85295, 85296, 85224, 85212, 85281, \n",
    "                                                85323, 85042, 85254, 85353, 85249, 85205, 85051, 85283, 85015, 85226, \n",
    "                                                85202, 85255, 85382, 85086, 85017, 85044, 85209, 85374, 85213, 85021, \n",
    "                                                85210, 85298, 85251, 85027, 85297, 85260, 85392, 85233, 85303, 85302, \n",
    "                                                85203, 85340, 85208, 85018, 85248, 85206, 85043, 85395, 85016, 85023, \n",
    "                                                85040, 85335, 85048, 85388, 85020, 85031, 85331, 85050, 85257, 85351, \n",
    "                                                85375, 85019, 85053, 85304, 85381, 85014, 85396, 85085, 85024, 85006, \n",
    "                                                85306, 85258, 85268, 85259, 85284, 85013, 85139, 85028, 85310, \n",
    "                                                85083, 85373, 85253, 85215, 85250, 85387, 85007, 85305, 85355, 85307, \n",
    "                                                85262, 85266, 85378, 85004, 85087, 85054, 85012, 85361, 85003, \n",
    "                                                85390, 85354, 85256, 85034, 85045, 85363, \n",
    "                                                85337, 85263, 85264, 85320, 85322, 85329, 85309, 85287, \n",
    "                                                85333, 85377, \n",
    "                                                85380, 85385, 85318, 85327, 85343, 85358, 85372, 85376, \n",
    "                                                85236, 85244, 85246, 85252, 85261, 85267, 85271, 85269, 85275, 85274, \n",
    "                                                85277, 85280, 85285, 85299, 85311, 85312, 85001, 85002, 85005, 85011, \n",
    "                                                85010, 85026, 85030, 85036, 85039, 85038, 85046, 85060, 85062, 85061, \n",
    "                                                85064, 85063, 85066, 85065, 85068, 85067, 85070, 85069, 85072, 85071, \n",
    "                                                85074, 85073, 85076, 85075, 85078, 85080, 85079, 85082, 85211, 85214, \n",
    "                                                85216, 85190, 85127]},\n",
    "    \n",
    "    \"Mohave\": {\"fips\": \"04015\", \"zip_codes\": [86442, 86401, 86409, 86406, 86403, 86404, 86426, 86413, 86440, 86429, \n",
    "                                              86021, 86432, 86441, 86434, 86444, 85360, 86436, 86443, 86445, \n",
    "                                              86411, 86438, 86437, 86431, 86405, 86412, 86427, 86430, 86433, 86439, \n",
    "                                              86446, 86402]},\n",
    "    \n",
    "    \"Navajo\": {\"fips\": \"04017\", \"zip_codes\": [85901, 86047, 85929, 85937, 86033, 86025, 85928, 86510, \n",
    "                                              85935, 86039, 86520, 86034, 86031, 86054, 85911, 85933, 86032, \n",
    "                                              85926, 86042, 85942, 85934, 86029, 86043, 85902, 85912, \n",
    "                                              85923, 85939]},\n",
    "    \n",
    "    \"Pima\": {\"fips\": \"04019\", \"zip_codes\": [85705, 85706, 85710, 85719, 85746, 85713, 85711, 85756, 85745, 85730, \n",
    "                                            85741, 85704, 85629, 85716, 85743, 85712, 85742, 85747, 85718, 85641, \n",
    "                                            85750, 85614, 85737, 85715, 85653, 85757, 85748, 85749, 85755, 85714, \n",
    "                                            85658, 85739, 85735, 85602, 85634, 85736, 85622, 85701, 85611, \n",
    "                                            85708, 85707, 85645, 85619, 85637, 85723, 85744, 85633, \n",
    "                                            85639, 85601, 85341, 85734, 85738, 85740, 85751, 85754, 85752, \n",
    "                                            85775, 85652, 85654, 85703, 85702, 85717, 85721, 85720, 85722, 85725, \n",
    "                                            85728, 85726, 85731, 85733, 85732]},\n",
    "    \n",
    "    \"Pinal\": {\"fips\": \"04021\", \"zip_codes\": [85142, 85122, 85212, 85138, 85143, 85132, 85140, \n",
    "                                             85248, 85120, 85119, 85131, 85139, 85128, 85658, 85739, \n",
    "                                             85118, 85123, 85194, 85193, 85623, 85631, 85539, \n",
    "                                             85173, 85137, 85147, 85618, 85192, 85172, \n",
    "                                             85191, 85141, 85121, 85130, \n",
    "                                             85117, 85178, 85145]},\n",
    "    \n",
    "    \"Santa Cruz\": {\"fips\": \"04023\", \"zip_codes\": [85621, 85648, 85611, 85624, 85646, 85637, 85640, \n",
    "                                                  85628]},\n",
    "    \n",
    "    \"Yavapai\": {\"fips\": \"04025\", \"zip_codes\": [86314, 86326, 86301, 86305, 86303, 86323, 86322, 86336, 86315, 86327, \n",
    "                                               85390, 86351, 86333, 86325, 86334, 86335, 86324, 85324, 86332, \n",
    "                                               86321, 86337, 85332, 86338, 86343, 85362, \n",
    "                                               86331, 86329, 86302, 86304, 86312, 86313, 86340, 86342, 86341]},\n",
    "    \n",
    "    \"Yuma\": {\"fips\": \"04027\", \"zip_codes\": [85364, 85365, 85350, 85349, 85367, 85356, 85347, 85333, 85336, 85352, \n",
    "                                            85366, 85369]}\n",
    "}\n",
    "\n",
    "# List to collect data and track failed requests\n",
    "all_data = []\n",
    "failed_requests = []\n",
    "\n",
    "# Headers for the API request\n",
    "headers = {'content-type': 'application/json'}\n",
    "\n",
    "# Data retrieval for each county, ZIP code, and year\n",
    "for county, info in counties.items():\n",
    "    fips = info['fips']\n",
    "    for zipcode in info['zip_codes']:\n",
    "        for year in range(2016, 2024):  # Iterates from 2016 to 2024\n",
    "            attempts = 0\n",
    "            while attempts < max_retries:\n",
    "                try:\n",
    "                    print(f\"Attempting data retrieval: {county}, ZIP: {zipcode}, Year: {year}, Attempt: {attempts + 1}\")\n",
    "                    payload = {\n",
    "                        \"household\": {\"people\": [{\"age\": 22, \"gender\": \"Male\"}]},\n",
    "                        \"market\": \"Individual\",\n",
    "                        \"place\": {\"countyfips\": fips, \"state\": \"AZ\", \"zipcode\": str(zipcode)},\n",
    "                        \"year\": year\n",
    "                    }\n",
    "                    response = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "                    if response.status_code == 200:\n",
    "                        plans = response.json().get('plans', [])\n",
    "                        for plan in plans:\n",
    "                            plan['Year'] = year  # Store the year within the plan's data\n",
    "                            plan['DB_ID'] = f\"{plan['id']}{zipcode}{year}\"\n",
    "                            all_data.append(plan)\n",
    "                        break  # Successful retrieval, exit retry loop\n",
    "                    else:\n",
    "                        print(f\"Failed data retrieval: {zipcode}, Year: {year}, Status: {response.status_code}, Error: {response.text}\")\n",
    "                        attempts += 1\n",
    "                        time.sleep(1)  # Sleep before retry\n",
    "                except requests.exceptions.ConnectTimeout:\n",
    "                    print(f\"Timeout for ZIP: {zipcode}, Year: {year}\")\n",
    "                    attempts += 1\n",
    "                    time.sleep(1)  # Sleep before retry\n",
    "\n",
    "# Convert collected data to DataFrame and handle duplicates\n",
    "big_data_df_3 = pd.DataFrame(all_data)\n",
    "big_data_df_3.drop_duplicates(subset='DB_ID', inplace=True)  # Ensure no duplicate entries based on the unique DB_ID\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify data integrity\n",
    "print(big_data_df_3.head())\n",
    "\n",
    "# Print any failed requests for debugging purposes\n",
    "print(\"Failed requests:\", failed_requests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bdeab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Define your database connection details\n",
    "DATABASE_URL = 'mysql+pymysql://admin:isba_4715@isba-dev-01.ctqaesoyaych.us-east-1.rds.amazonaws.com:3306/SQL_Project_Spring_2024'\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Assuming big_data_df_3 is your DataFrame loaded with API data\n",
    "# Extract the necessary columns including the unique DB_ID and ensure column names match exactly those in your DataFrame\n",
    "plans_data = big_data_df_3[['DB_ID', 'name', 'metal_level', 'type', 'premium', 'premium_w_credit', 'ehb_premium', 'Year']]\n",
    "plans_data.columns = ['db_id', 'name', 'metal_level', 'plan_type', 'premium', 'premium_with_credit', 'ehb_premium', 'year']\n",
    "\n",
    "# Upload the data to the Plans table\n",
    "try:\n",
    "    plans_data.to_sql('Plans', con=engine, if_exists='append', index=False)\n",
    "    print(\"Data uploaded successfully to the Plans table.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred during the data upload:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae4e808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This script defines database connection details and extracts necessary columns from a DataFrame loaded with API data.\n",
    "# It renames the columns to match the database schema and uploads the data to the Plans table.\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Define your database connection details\n",
    "DATABASE_URL = 'mysql+pymysql://admin:isba_4715@isba-dev-01.ctqaesoyaych.us-east-1.rds.amazonaws.com:3306/SQL_Project_Spring_2024'\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Assuming big_data_df is your DataFrame loaded with API data\n",
    "# Extract the necessary columns including the unique DB_ID\n",
    "plans_data = big_data_df_3[['DB_ID', 'Name', 'Metal Level', 'Type', 'Product Division', 'Premium', 'Premium w/Credit', 'EHB Premium', 'Year']]\n",
    "plans_data.columns = ['db_id', 'name', 'metal_level', 'plan_type', 'product_division', 'premium', 'premium_with_credit', 'ehb_premium', 'year']\n",
    "\n",
    "# Upload the data to the Plans table\n",
    "try:\n",
    "    plans_data.to_sql('Plans', con=engine, if_exists='append', index=False)\n",
    "    print(\"Data uploaded successfully to the Plans table.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred during the data upload:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e59dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script defines database connection details and mappings for county names and FIPS codes.\n",
    "# It creates a DataFrame from the county FIPS code dictionary.\n",
    "# Data extraction setup is performed for Deductibles, MOOPs, and Benefits data from 'big_data_df_3'.\n",
    "# Finally, it defines a function to upload data to the database and uploads data to each specified table.\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Define your database connection details\n",
    "DATABASE_URL = 'mysql+pymysql://admin:isba_4715@isba-dev-01.ctqaesoyaych.us-east-1.rds.amazonaws.com:3306/SQL_Project_Spring_2024'\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Define the county and FIPS code mappings\n",
    "county_fips = {\n",
    "    'Apache': '04001',\n",
    "    'Cochise': '04003',\n",
    "    'Coconino': '04005',\n",
    "    'Gila': '04007',\n",
    "    'Graham': '04009',\n",
    "    'Greenlee': '04011',\n",
    "    'La Paz': '04012',\n",
    "    'Maricopa': '04013',\n",
    "    'Mohave': '04015',\n",
    "    'Navajo': '04017',\n",
    "    'Pima': '04019',\n",
    "    'Pinal': '04021',\n",
    "    'Santa Cruz': '04023',\n",
    "    'Yavapai': '04025',\n",
    "    'Yuma': '04027'\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "counties_data = pd.DataFrame(list(county_fips.items()), columns=['name', 'fips_code'])\n",
    "\n",
    "# Assuming big_data_df_3 is your DataFrame loaded with API data\n",
    "# Example data extraction setup for Deductibles, MOOPs, and Benefits data\n",
    "deductibles_data = big_data_df_3[['DB_ID', 'Deductible Type', 'Deductible Amount']].copy()\n",
    "deductibles_data.columns = ['db_id', 'deductible_type', 'amount']\n",
    "\n",
    "moops_data = big_data_df_3[['DB_ID', 'MOOP Type', 'MOOP Amount']].copy()\n",
    "moops_data.columns = ['db_id', 'moop_type', 'amount']\n",
    "\n",
    "\n",
    "# Function to upload data to the database\n",
    "def upload_data(dataframe, table_name):\n",
    "    try:\n",
    "        dataframe.to_sql(table_name, con=engine, if_exists='append', index=False)\n",
    "        print(f\"Data uploaded successfully to the {table_name} table.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the data upload to {table_name}:\", e)\n",
    "\n",
    "# Upload data to each specified table\n",
    "upload_data(counties_data, 'Counties')\n",
    "upload_data(deductibles_data, 'Deductibles')\n",
    "upload_data(moops_data, 'MOOPs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310947f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After collecting data into 'all_data', a DataFrame 'big_data_df_4' is created from it.\n",
    "# Duplicates are removed from 'big_data_df_4' based on the 'DB_ID' column.\n",
    "# The original number of entries and the number of entries after removing duplicates are printed.\n",
    "# The resulting DataFrame is saved to a CSV file located at '/Users/aidantaggartpersonal/Downloads/my_data_4.csv'.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# After data collection\n",
    "big_data_df_4 = pd.DataFrame(all_data)\n",
    "print(\"Original number of entries:\", len(big_data_df_3))\n",
    "big_data_df_4 = big_data_df_4.drop_duplicates(subset=['DB_ID'])\n",
    "# Remove duplicates\n",
    "print(\"Number of entries after removing duplicates:\", len(big_data_df_3))\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "file_path = '/Users/aidantaggartpersonal/Downloads/my_data_4.csv'\n",
    "big_data_df_4.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49239ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block processes data from a DataFrame containing benefits information, \n",
    "#checks the format of the 'Benefits' column, converts string representations to actual \n",
    "#lists of dictionaries if necessary, iterates over each row to extract benefit data, \n",
    "#creates a list of dictionaries for all benefits, converts the list to a DataFrame, \n",
    "#and uploads the data to a database table.\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import ast  # Import ast to safely evaluate string literals as Python expressions\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define your database connection details\n",
    "DATABASE_URL = 'mysql+pymysql://admin:isba_4715@isba-dev-01.ctqaesoyaych.us-east-1.rds.amazonaws.com:3306/SQL_Project_Spring_2024'\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Assuming big_data_df_3 is your DataFrame loaded with API data\n",
    "logging.info(\"Checking the format of the 'Benefits' column.\")\n",
    "# Check if the 'Benefits' column needs parsing from string to actual list of dicts\n",
    "if isinstance(big_data_df_3.loc[0, 'Benefits'], str):\n",
    "    big_data_df_3['Benefits'] = big_data_df_3['Benefits'].apply(ast.literal_eval)\n",
    "    logging.info(\"Converted string representations to actual lists of dictionaries.\")\n",
    "\n",
    "# Iterate over each row in the DataFrame to process the 'Benefits' column\n",
    "all_benefits = []\n",
    "logging.info(\"Starting to process each row for benefits data extraction.\")\n",
    "for index, row in big_data_df_3.iterrows():\n",
    "    db_id = row['DB_ID']\n",
    "    for benefit in row['Benefits']:  # Processing each benefit in the list of dictionaries\n",
    "        benefit_data = {\n",
    "            'db_id': db_id,\n",
    "            'name': benefit.get('Benefit Name'),\n",
    "            'covered': benefit.get('Covered'),\n",
    "            'coinsurance_rate': benefit.get('Coinsurance Rate'),\n",
    "            'coinsurance_options': benefit.get('Coinsurance Options'),\n",
    "            'copay_amount': benefit.get('Copay Amount'),\n",
    "            'copay_options': benefit.get('Copay Options'),\n",
    "            'network_tier': benefit.get('Network Tier'),\n",
    "            'cost_sharing_reduction': benefit.get('Cost Sharing Reduction'),\n",
    "            'display_string': benefit.get('Display String'),\n",
    "            'has_limits': benefit.get('Has Limits'),\n",
    "            'limit_unit': benefit.get('Limit Unit'),\n",
    "            'limit_quantity': benefit.get('Limit Quantity')\n",
    "        }\n",
    "        all_benefits.append(benefit_data)\n",
    "logging.info(\"Finished processing benefits data for all rows.\")\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "benefits_df = pd.DataFrame(all_benefits)\n",
    "logging.info(\"Converted all benefits data into DataFrame.\")\n",
    "\n",
    "# Function to upload data to the database\n",
    "def upload_data(dataframe, table_name):\n",
    "    try:\n",
    "        dataframe.to_sql(table_name, con=engine, if_exists='append', index=False)\n",
    "        logging.info(f\"Data uploaded successfully to the {table_name} table.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during the data upload to {table_name}:\", exc_info=True)\n",
    "\n",
    "# Upload benefits data to the Benefits table\n",
    "upload_data(benefits_df, 'Benefits')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbe9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/aidantaggartpersonal/Downloads/my_data_4.csv')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d1ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block loads a CSV file containing data on benefits, parses the 'Benefits' column, explodes the lists within that column into separate rows, and extracts data from the dictionaries in the 'Benefits' column into separate columns. \n",
    "# It then selects and renames columns to match specified requirements and saves the final DataFrame to a CSV file. Finally, it displays the first few rows of the final DataFrame.\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load the CSV file from the uploaded location\n",
    "logging.info('Loading CSV file...')\n",
    "df = pd.read_csv('/Users/aidantaggartpersonal/Downloads/my_data_4.csv')\n",
    "\n",
    "# Check if the 'Benefits' column is a string of lists and parse it safely\n",
    "logging.info('Parsing the Benefits column...')\n",
    "df['Benefits'] = df['Benefits'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Explode the 'Benefits' list into separate rows\n",
    "logging.info('Exploding Benefits into separate rows...')\n",
    "df_exploded = df.explode('Benefits')\n",
    "\n",
    "# Extract data from the dictionary in 'Benefits' into separate columns\n",
    "benefits_columns = ['name', 'covered', 'coinsurance_rate', 'coinsurance_options', 'copay_amount', \n",
    "                    'copay_options', 'network_tier', 'cost_sharing_reduction', 'display_string', \n",
    "                    'has_limits', 'limit_unit', 'limit_quantity']\n",
    "logging.info('Extracting data from Benefits into separate columns...')\n",
    "for col in benefits_columns:\n",
    "    df_exploded[col] = df_exploded['Benefits'].apply(lambda x: x.get(col))\n",
    "\n",
    "# Select and rename columns to match your requirements\n",
    "df_exploded = df_exploded[['DB_ID'] + benefits_columns].copy()  # Using .copy() to ensure we're working with a copy\n",
    "df_exploded.rename(columns={'name': 'Benefit Name'}, inplace=True)\n",
    "\n",
    "# Save the final DataFrame\n",
    "logging.info('Saving the parsed data to CSV...')\n",
    "df_exploded.to_csv('/Users/aidantaggartpersonal/Downloads/parsed_benefits.csv', index=False)\n",
    "\n",
    "# Display the first few rows to confirm\n",
    "logging.info('Displaying the first few rows of the final DataFrame:')\n",
    "print(df_exploded.iloc[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3415d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block processes a CSV file containing data on benefits. It loads the CSV file, checks if the 'Benefits' column contains string representations of lists, safely parses them into lists, and explodes the lists into separate rows. \n",
    "# Then, it extracts data from the dictionaries within the 'Benefits' column into separate columns, renames the columns to match the database schema, selects relevant columns for database upload, and uploads the data to the database table named 'Benefits'.\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define your database connection details\n",
    "DATABASE_URL = 'mysql+pymysql://admin:isba_4715@isba-dev-01.ctqaesoyaych.us-east-1.rds.amazonaws.com:3306/SQL_Project_Spring_2024'\n",
    "engine = create_engine(DATABASE_URL)\n",
    "logging.info('Database engine created.')\n",
    "\n",
    "# Load the CSV file from the uploaded location\n",
    "logging.info('Loading CSV file...')\n",
    "df = pd.read_csv('/Users/aidantaggartpersonal/Downloads/my_data_4.csv')\n",
    "\n",
    "# Check if the 'Benefits' column is a string of lists and parse it safely\n",
    "logging.info('Parsing the Benefits column...')\n",
    "df['Benefits'] = df['Benefits'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Explode the 'Benefits' list into separate rows\n",
    "logging.info('Exploding Benefits into separate rows...')\n",
    "df_exploded = df.explode('Benefits')\n",
    "\n",
    "# Extract data from the dictionary in 'Benefits' into separate columns\n",
    "benefits_columns = ['name', 'covered', 'coinsurance_rate', 'coinsurance_options', 'copay_amount', \n",
    "                    'copay_options', 'network_tier', 'cost_sharing_reduction', 'display_string', \n",
    "                    'has_limits', 'limit_unit', 'limit_quantity']\n",
    "logging.info('Extracting data from Benefits into separate columns...')\n",
    "for col in benefits_columns:\n",
    "    df_exploded[col] = df_exploded['Benefits'].apply(lambda x: x.get(col))\n",
    "\n",
    "# Rename columns to match database schema\n",
    "df_exploded.rename(columns={\n",
    "    'name': 'name',\n",
    "    'covered': 'covered',\n",
    "    'coinsurance_rate': 'coinsurance_rate',\n",
    "    'coinsurance_options': 'coinsurance_options',\n",
    "    'copay_amount': 'copay_amount',\n",
    "    'copay_options': 'copay_options',\n",
    "    'network_tier': 'network_tier',\n",
    "    'cost_sharing_reduction': 'cost_sharing_reduction',\n",
    "    'display_string': 'display_string',\n",
    "    'has_limits': 'has_limits',\n",
    "    'limit_unit': 'limit_unit',\n",
    "    'limit_quantity': 'limit_quantity'\n",
    "}, inplace=True)\n",
    "\n",
    "# Select columns for the database upload\n",
    "final_df = df_exploded[['DB_ID', 'name', 'covered', 'coinsurance_rate', 'coinsurance_options', 'copay_amount', \n",
    "                        'copay_options', 'network_tier', 'cost_sharing_reduction', 'display_string', \n",
    "                        'has_limits', 'limit_unit', 'limit_quantity']].iloc[100]\n",
    "\n",
    "# Upload the data to the database\n",
    "logging.info('Uploading data to the database...')\n",
    "final_df.to_sql('Benefits', con=engine, if_exists='append', index=False)\n",
    "logging.info('Data successfully uploaded to the database.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5bd5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block processes a DataFrame loaded from a CSV file. It removes duplicates based on a specific column,\n",
    "# prepares two DataFrames ('ZipCodes' and 'PlanZipCodes'), establishes a connection to a database, and uploads the\n",
    "# prepared DataFrames to the respective tables in the database.\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Assume big_data_df_3 is already loaded and available\n",
    "df = pd.read_csv('/Users/aidantaggartpersonal/Downloads/my_data_4.csv')\n",
    "\n",
    "# Print the column names to verify what's available\n",
    "print(\"Column names in the DataFrame:\", df.columns)\n",
    "\n",
    "# Ensure the correct column names are used, checking for 'Zipcode' and 'County FIPS'\n",
    "if 'Zipcode' in df.columns and 'County_FIPS' in df.columns:\n",
    "    # Remove duplicates based on 'DB_ID'\n",
    "    df = df.drop_duplicates(subset=['DB_ID'])\n",
    "    print(\"Number of entries after removing duplicates:\", len(df))\n",
    "\n",
    "    # Prepare ZipCodes DataFrame\n",
    "    zip_codes_df = df[['Zipcode', 'County_FIPS']].drop_duplicates()\n",
    "    zip_codes_df['county_id'] = zip_codes_df['County_FIPS'].factorize()[0] + 1  # Creating county_ids starting from 1\n",
    "    zip_codes_df.rename(columns={'Zipcode': 'zip_code'}, inplace=True)\n",
    "\n",
    "    # Prepare PlanZipCodes DataFrame\n",
    "    plan_zip_codes_df = df[['DB_ID', 'Zipcode']].drop_duplicates()\n",
    "    plan_zip_codes_df.rename(columns={'Zipcode': 'zip_id'}, inplace=True)\n",
    "\n",
    "    # Establish database connection\n",
    "    DATABASE_URL = 'mysql+pymysql://admin:isba_4715@isba-dev-01.ctqaesoyaych.us-east-1.rds.amazonaws.com:3306/SQL_Project_Spring_2024'\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "\n",
    "    # Upload the DataFrame to your database\n",
    "    zip_codes_df.to_sql('ZipCodes', con=engine, if_exists='append', index=False)\n",
    "    plan_zip_codes_df.to_sql('PlanZipCodes', con=engine, if_exists='append', index=False)\n",
    "\n",
    "    print(\"Data successfully uploaded to the database for both ZipCodes and PlanZipCodes tables.\")\n",
    "else:\n",
    "    print(\"Required columns 'Zipcode' and 'County FIPS' not found in DataFrame. Please check your DataFrame columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd04d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block reads a DataFrame from a CSV file, extracts unique zip codes and County FIPS codes, and creates a mapping\n",
    "# between County FIPS codes and county IDs. It then prints the number of unique zip codes and County FIPS codes found,\n",
    "# followed by a detailed mapping showing each zip code, its corresponding County FIPS code, and the matched county ID.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the DataFrame from the CSV file\n",
    "df = pd.read_csv('/Users/aidantaggartpersonal/Downloads/my_data_4.csv')\n",
    "print(\"Data read successfully. Columns available:\", df.columns.tolist())\n",
    "\n",
    "# Extract unique zip codes and County FIPS codes\n",
    "unique_zipcodes = df['Zipcode'].unique()\n",
    "unique_county_fips = df['County FIPS'].unique()\n",
    "\n",
    "print(f\"Found {len(unique_zipcodes)} unique zip codes and {len(unique_county_fips)} unique county FIPS codes.\")\n",
    "\n",
    "# Define the mapping between County FIPS codes and county IDs\n",
    "county_fips_to_id = {\n",
    "    4001: 1, 4003: 2, 4005: 3, 4007: 4, 4009: 5,\n",
    "    4011: 6, 4012: 7, 4013: 8, 4015: 9, 4017: 10,\n",
    "    4019: 11, 4021: 12, 4023: 13, 4025: 14, 4027: 15\n",
    "}\n",
    "\n",
    "# Prepare to show zip code, county FIPS, and county ID mapping\n",
    "mapping_details = []\n",
    "for zip_code, county_fips in zip(unique_zipcodes, unique_county_fips):\n",
    "    county_id = county_fips_to_id.get(county_fips, \"No matching county ID\")\n",
    "    mapping_details.append((zip_code, county_fips, county_id))\n",
    "\n",
    "# Print detailed mapping\n",
    "for detail in mapping_details:\n",
    "    print(f\"Zip Code: {detail[0]}, County FIPS: {detail[1]}, County ID: {detail[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a83b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block reads a DataFrame from a CSV file, processes it, and creates a new DataFrame with specific columns.\n",
    "# It defines a mapping between County FIPS codes and county IDs, prepares data for the new DataFrame by iterating through\n",
    "# the original DataFrame rows, and creates a new DataFrame with columns 'zip_code', 'fibs_code', and 'county_id'.\n",
    "# The final DataFrame is then printed to check the data, along with the total number of rows.\n",
    "# Another DataFrame containing only 'zip_code' and 'county_id' columns is created and printed for further verification.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the DataFrame from the CSV file\n",
    "df = pd.read_csv('/Users/aidantaggartpersonal/Downloads/my_data_4.csv')\n",
    "print(\"Data read successfully. Columns available:\", df.columns.tolist())\n",
    "\n",
    "# Define the mapping between County FIPS codes and county IDs\n",
    "county_fips_to_id = {\n",
    "    4001: 1, 4003: 2, 4005: 3, 4007: 4, 4009: 5,\n",
    "    4011: 6, 4012: 7, 4013: 8, 4015: 9, 4017: 10,\n",
    "    4019: 11, 4021: 12, 4023: 13, 4025: 14, 4027: 15\n",
    "}\n",
    "\n",
    "# Prepare data for the new DataFrame\n",
    "data_for_dataframe = {}\n",
    "for index, row in df.iterrows():\n",
    "    zip_code = row['Zipcode']\n",
    "    county_fips = row['County FIPS']\n",
    "    # Check if zip_code is already processed to avoid duplication\n",
    "    if zip_code not in data_for_dataframe:\n",
    "        county_id = county_fips_to_id.get(county_fips, None)\n",
    "        data_for_dataframe[zip_code] = {'zip_code': zip_code, 'fibs_code': county_fips, 'county_id': county_id}\n",
    "\n",
    "# Create a list of dictionaries for DataFrame conversion\n",
    "final_data = list(data_for_dataframe.values())\n",
    "\n",
    "# Create the DataFrame\n",
    "results_df = pd.DataFrame(final_data)\n",
    "\n",
    "# Print the new DataFrame to check the data\n",
    "print(results_df)\n",
    "\n",
    "# Print the total number of rows in the DataFrame\n",
    "print(f\"Total rows in the new DataFrame: {len(results_df)}\")\n",
    "\n",
    "# Create another DataFrame that only contains zip_code and county_id\n",
    "zip_county_df = results_df[['zip_code', 'county_id']].copy()\n",
    "\n",
    "# Print the new DataFrame to check the data\n",
    "print(zip_county_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549db4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block reads a DataFrame from a CSV file and prepares it for upload to a MySQL database.\n",
    "# It sets up logging configuration for debugging purposes, reads the DataFrame from the CSV file,\n",
    "# extracts the required columns ('zip_code' and 'county_id'), and creates a database connection using SQLAlchemy.\n",
    "# Then, it attempts to upload the DataFrame to the 'ZipCodes' table in the specified database schema.\n",
    "# If successful, it logs a success message; otherwise, it logs an error message along with the exception details.\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, types\n",
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Read the DataFrame from the CSV file and prepare it\n",
    "# This is assuming 'results_df' is directly available in this script; otherwise, include its creation here\n",
    "logging.info(\"Preparing DataFrame with the required columns.\")\n",
    "\n",
    "# Extract only the 'zip_code' and 'county_id' columns\n",
    "zip_county_df = results_df[['zip_code', 'county_id']].copy()\n",
    "logging.debug(f\"Prepared DataFrame for upload:\\n{zip_county_df.head()}\")\n",
    "\n",
    "# Create the database connection\n",
    "DATABASE_URL = 'mysql+pymysql://admin:isba_4715@isba-dev-01.ctqaesoyaych.us-east-1.rds.amazonaws.com:3306/SQL_Project_Spring_2024'\n",
    "engine = create_engine(DATABASE_URL)\n",
    "logging.info(\"Database connection established.\")\n",
    "\n",
    "# Upload the DataFrame to the database\n",
    "try:\n",
    "    zip_county_df.to_sql('ZipCodes', con=engine, schema='SQL_Project_Spring_2024', if_exists='append', index=False,\n",
    "                         dtype={\n",
    "                             'zip_code': types.VARCHAR(length=10),\n",
    "                             'county_id': types.INTEGER()\n",
    "                         })\n",
    "    logging.info(\"Data successfully uploaded to the database for the ZipCodes table.\")\n",
    "except Exception as e:\n",
    "    logging.error(\"Failed to upload data to the database.\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block establishes a connection to a MySQL database using SQLAlchemy and performs operations to manage data integrity.\n",
    "# It sets up logging for debugging purposes and creates a database connection using the provided database URL.\n",
    "# Then, it creates a session using sessionmaker and binds it to the engine.\n",
    "# Inside the try block, it disables foreign key checks to bypass dependencies temporarily, deletes all data from the ZipCodes table,\n",
    "# and then re-enables foreign key checks to ensure data integrity.\n",
    "# It commits the transaction if successful and logs the changes.\n",
    "# If any exception occurs during the process, it rolls back the transaction, logs an error, and closes the session.\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, types\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.sql import text\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Create the database connection\n",
    "DATABASE_URL = 'mysql+pymysql://admin:isba_4715@isba-dev-01.ctqaesoyaych.us-east-1.rds.amazonaws.com:3306/SQL_Project_Spring_2024'\n",
    "engine = create_engine(DATABASE_URL)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "try:\n",
    "    # Disable foreign key checks to bypass dependencies\n",
    "    session.execute(text(\"SET FOREIGN_KEY_CHECKS=0;\"))\n",
    "    logging.info(\"Foreign key checks disabled.\")\n",
    "\n",
    "    # Delete all data from the ZipCodes table\n",
    "    session.execute(text(\"DELETE FROM ZipCodes;\"))\n",
    "    logging.info(\"All data deleted from ZipCodes table.\")\n",
    "\n",
    "    # Re-enable foreign key checks\n",
    "    session.execute(text(\"SET FOREIGN_KEY_CHECKS=1;\"))\n",
    "    logging.info(\"Foreign key checks re-enabled.\")\n",
    "\n",
    "    # Commit the transaction\n",
    "    session.commit()\n",
    "    logging.info(\"Changes committed to the database.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Roll back the transaction in case of an error\n",
    "    session.rollback()\n",
    "    logging.error(\"Transaction rolled back due to an error.\", exc_info=True)\n",
    "finally:\n",
    "    # Close the session\n",
    "    session.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae3ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code establishes a connection to a MySQL database using SQLAlchemy and uploads data from a DataFrame to the database table.\n",
    "# It sets up logging for debugging purposes and creates a database connection using the provided database URL.\n",
    "# Then, it creates a session using sessionmaker and binds it to the engine.\n",
    "# Inside the try block, it checks if the county_ids in the DataFrame to be uploaded exist in the Counties table of the database.\n",
    "# If any county_id does not exist, it logs a warning and removes those records from the DataFrame.\n",
    "# Next, it selects only the necessary columns from the DataFrame for insertion.\n",
    "# After preparing the data, it inserts the DataFrame into the ZipCodes table of the database.\n",
    "# It commits the transaction if successful and logs a success message.\n",
    "# If any exception occurs during the process, it rolls back the transaction, logs an error, and closes the session.\n",
    "# Overall, this code block ensures data integrity by validating county_ids before insertion and provides logging for debugging purposes.\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, types\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Create the database connection\n",
    "DATABASE_URL = 'mysql+pymysql://admin:isba_4715@isba-dev-01.ctqaesoyaych.us-east-1.rds.amazonaws.com:3306/SQL_Project_Spring_2024'\n",
    "engine = create_engine(DATABASE_URL)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "try:\n",
    "    # Assuming 'zip_county_df' is prepared and contains the data to be uploaded\n",
    "    # First, check that all county_ids exist in the Counties table\n",
    "    valid_county_ids = set(pd.read_sql(\"SELECT county_id FROM Counties\", con=session.bind)['county_id'])\n",
    "    zip_county_df['valid'] = zip_county_df['county_id'].apply(lambda x: x in valid_county_ids)\n",
    "    \n",
    "    if not zip_county_df['valid'].all():\n",
    "        logging.warning(\"Some county_ids do not exist in the Counties table. These records will not be inserted.\")\n",
    "        zip_county_df = zip_county_df[zip_county_df['valid'] == True]\n",
    "\n",
    "    zip_county_df = zip_county_df[['zip_code', 'county_id']]  # Ensure only necessary columns are present\n",
    "\n",
    "    # Perform the insertion\n",
    "    zip_county_df.to_sql('ZipCodes', con=session.bind, schema='SQL_Project_Spring_2024', if_exists='append', index=False,\n",
    "                         dtype={\n",
    "                             'zip_code': types.VARCHAR(length=10),\n",
    "                             'county_id': types.INTEGER()\n",
    "                         })\n",
    "    session.commit()\n",
    "    logging.info(\"Data successfully uploaded to the database for the ZipCodes table.\")\n",
    "\n",
    "except Exception as e:\n",
    "    session.rollback()\n",
    "    logging.error(\"Transaction rolled back due to an error while uploading data.\", exc_info=True)\n",
    "finally:\n",
    "    session.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9aa75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code reads a DataFrame from a CSV file and performs data manipulation to prepare a new DataFrame.\n",
    "# It starts by reading the DataFrame from the specified CSV file and prints the columns available in the DataFrame.\n",
    "# Then, it defines a mapping between County FIPS codes and county IDs.\n",
    "# Next, it iterates over each row in the DataFrame, extracts relevant data, and prepares it for the new DataFrame.\n",
    "# After preparing the data, it creates a new DataFrame using the collected data.\n",
    "# The new DataFrame is printed to check the data and the total number of rows.\n",
    "# It then creates another DataFrame containing only 'db_id', 'zip_code', and 'county_id' columns.\n",
    "# Finally, it writes the new DataFrame to a CSV file and prints the path where the data is saved.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the DataFrame from the CSV file\n",
    "df = pd.read_csv('/Users/aidantaggartpersonal/Downloads/my_data_4.csv')\n",
    "print(\"Data read successfully. Columns available:\", df.columns.tolist())\n",
    "\n",
    "# Define the mapping between County FIPS codes and county IDs\n",
    "county_fips_to_id = {\n",
    "    4001: 1, 4003: 2, 4005: 3, 4007: 4, 4009: 5,\n",
    "    4011: 6, 4012: 7, 4013: 8, 4015: 9, 4017: 10,\n",
    "    4019: 11, 4021: 12, 4023: 13, 4025: 14, 4027: 15\n",
    "}\n",
    "\n",
    "# Prepare data for the new DataFrame\n",
    "data_for_dataframe = {}\n",
    "for index, row in df.iterrows():\n",
    "    db_id = row['DB_ID']\n",
    "    zip_code = row['Zipcode']\n",
    "    county_fips = row['County FIPS']\n",
    "    # Check if zip_code is already processed to avoid duplication\n",
    "    if zip_code not in data_for_dataframe:\n",
    "        county_id = county_fips_to_id.get(county_fips, None)\n",
    "        data_for_dataframe[zip_code] = {'db_id': db_id, 'zip_code': zip_code, 'fibs_code': county_fips, 'county_id': county_id}\n",
    "\n",
    "# Create a list of dictionaries for DataFrame conversion\n",
    "final_data = list(data_for_dataframe.values())\n",
    "\n",
    "# Create the DataFrame\n",
    "results_df = pd.DataFrame(final_data)\n",
    "\n",
    "# Print the new DataFrame to check the data\n",
    "print(results_df)\n",
    "\n",
    "# Print the total number of rows in the new DataFrame\n",
    "print(f\"Total rows in the new DataFrame: {len(results_df)}\")\n",
    "\n",
    "# Create another DataFrame that contains db_id, zip_code, and county_id\n",
    "zip_db_county_df = results_df[['db_id', 'zip_code', 'county_id']].copy()\n",
    "\n",
    "# Print the new DataFrame to check the data\n",
    "print(zip_db_county_df)\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "output_path = '/Users/aidantaggartpersonal/Downloads/output_data.csv'\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"Data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11067aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code imports the pandas library as pd and creates a connection to a MySQL database using SQLAlchemy.\n",
    "# It defines the database URL which includes the necessary credentials and database name.\n",
    "# Then, it connects to the database using the create_engine function from SQLAlchemy.\n",
    "# Next, it defines a SQL query to select all rows from the ZipCodes table.\n",
    "# It executes the query using the read_sql function from pandas and loads the results into a DataFrame called zipcodes_df.\n",
    "# Finally, it prints the DataFrame to display the data retrieved from the ZipCodes table.\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Define the database URL\n",
    "DATABASE_URL = 'mysql+pymysql://admin:isba_4715@isba-dev-01.ctqaesoyaych.us-east-1.rds.amazonaws.com:3306/SQL_Project_Spring_2024'\n",
    "\n",
    "# Connect to the database\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Query to select all rows from the ZipCodes table\n",
    "query = \"SELECT * FROM ZipCodes\"\n",
    "\n",
    "# Execute the query and load the results into a DataFrame\n",
    "zipcodes_df = pd.read_sql(query, con=engine)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(zipcodes_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec42c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'zip_code' column in zip_db_county_df to string\n",
    "zip_db_county_df['zip_code'] = zip_db_county_df['zip_code'].astype(str)\n",
    "\n",
    "# Merge the DataFrames based on the common column 'zip_code' and 'county_id'\n",
    "combined_df = pd.merge(zipcodes_df, zip_db_county_df, on=['zip_code', 'county_id'], how='inner')\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf5e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with 'zip_id' and 'db_id' columns from combined_df\n",
    "zip_id_db_id_df = combined_df[['zip_id', 'db_id']]\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(zip_id_db_id_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25159f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sqlalchemy import create_engine, update, MetaData, Table, text\n",
    "from sqlalchemy.orm import Session\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure logging to file and console\n",
    "logging.basicConfig(filename='update.log', filemode='w', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console.setFormatter(formatter)\n",
    "logging.getLogger('').addHandler(console)\n",
    "\n",
    "# Database setup\n",
    "DATABASE_URL = 'mysql+pymysql://admin:isba_4715@isba-dev-01.ctqaesoyaych.us-east-1.rds.amazonaws.com:3306/SQL_Project_Spring_2024'\n",
    "engine = create_engine(DATABASE_URL)\n",
    "logging.info(\"Database engine created.\")\n",
    "\n",
    "# Load 'Plans' table metadata\n",
    "metadata = MetaData()\n",
    "plans_table = Table('Plans', metadata, autoload_with=engine)\n",
    "logging.info(\"Plans table metadata loaded.\")\n",
    "\n",
    "# Prepare DataFrame - ensure big_data_df_3 is defined with DB_ID and Year\n",
    "if 'big_data_df_3' in locals():\n",
    "    data_to_upload = big_data_df_3[['DB_ID', 'Year']]  # Assuming big_data_df_3 is already prepared\n",
    "    logging.info(f\"Data to upload prepared with {len(data_to_upload)} rows.\")\n",
    "else:\n",
    "    logging.error(\"DataFrame big_data_df_3 is not defined.\")\n",
    "    raise Exception(\"DataFrame big_data_df_3 is not defined.\")\n",
    "\n",
    "# Update operation with logging and batch processing\n",
    "with Session(engine) as session:\n",
    "    try:\n",
    "        # Disable foreign key checks\n",
    "        session.execute(text('SET foreign_key_checks = 0'))\n",
    "        logging.info(\"Foreign key checks disabled.\")\n",
    "\n",
    "        # Batch update process\n",
    "        batch_size = 10000  # Set to 5000 or 10000 as needed\n",
    "        for i in range(0, len(data_to_upload), batch_size):\n",
    "            batch_start_time = time.time()  # Start time for the batch\n",
    "            batch = data_to_upload.iloc[i:i+batch_size]\n",
    "            logging.info(f\"Processing batch {i//batch_size + 1} with {len(batch)} records.\")\n",
    "            # Build a list of update statements\n",
    "            updates = []\n",
    "            for index, row in batch.iterrows():\n",
    "                updates.append(\n",
    "                    update(plans_table)\n",
    "                    .where(plans_table.c.db_id == str(row['DB_ID']))\n",
    "                    .values(year=int(row['Year']))  # Ensure column name matches DB\n",
    "                )\n",
    "                # Log every 250 records within the batch\n",
    "                if (index + 1) % 250 == 0:\n",
    "                    logging.info(f\"Processed {index + 1} records of batch {i//batch_size + 1}.\")\n",
    "                    \n",
    "            # Execute updates in a batch\n",
    "            for stmt in updates:\n",
    "                session.execute(stmt)\n",
    "            session.commit()\n",
    "            batch_end_time = time.time()  # End time for the batch\n",
    "            logging.info(f\"Batch {i//batch_size + 1} committed successfully in {batch_end_time - batch_start_time:.2f} seconds.\")\n",
    "\n",
    "        # Re-enable foreign key checks\n",
    "        session.execute(text('SET foreign_key_checks = 1'))\n",
    "        logging.info(\"Foreign key checks re-enabled.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        logging.error(\"Failed to commit changes due to: %s\", str(e))\n",
    "        raise\n",
    "    finally:\n",
    "        # Ensure that foreign key checks are re-enabled in case of error\n",
    "        session.execute(text('SET foreign_key_checks = 1'))\n",
    "        session.close()\n",
    "\n",
    "logging.info(\"Database updates completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1f5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
